{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "Which of the following are the most accurate characterizations of sample models and distribution models?\n",
    "\n",
    "* Both sample models and distribution models can be used to obtain a possible next sate and reward, given te current state and action.\n",
    "* A distribution model can be used as a sample model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Which of the following statements are TRUE for Dyna architecture?\n",
    "\n",
    "* Real experience can be used to improve the value function and policy\n",
    "* Real experience can be used to improve the model\n",
    "* Simulated experience can be used to improve the value function and policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Mark all statements that are TRUE for the tabular Dyna-Q algorithm\n",
    "* For a given state-action pair, the model predicts the next state and reward\n",
    "* The environment is asssumennd to be deterministic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Which of the following statements are True? (ALL)\n",
    "\n",
    "* Model-based methods often suffer more from bias than model-free methods, because of innacuracies in the model.\n",
    "* The amoun of computation per interaction with the environment is larger in the Dyna-Q algorithm (with non-zero planning steps) as compared to the Q-learning algorithm.\n",
    "* Model-based methods like Dyna typically require more memory than model-free methods like Q-learning.\n",
    "* When compared with model-free methods, model based methods are relatively more sample efficient. They can achieve a comparable performance with comparatively fewer environmental interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Which of the following is generally the most computationally expensive step of Dyna-Q algorithm?\n",
    "* Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 What are some possible reasons for learned model to be innacurate?\n",
    "* The environment has changed\n",
    "* The transition dynamics of the environment are stochastic, and only few transitions have been experienced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In search control, which of the following methods is likely to make  Dyna agent perform better in problems with a large number of states? Recall that search control is the process that selects the starting and action in planning.\n",
    "\n",
    "* Select state-action pairs uniformly at random from all previously experienced pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Wich of the following are planning methods?\n",
    "* Dyna-Q\n",
    "* Value iteration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
