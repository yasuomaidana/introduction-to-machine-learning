Multiple layers of perceptrons with non-linear functions after each layer. Also called multilayer perceptrons.
- Can represent more complex decision boundaries
- They can compute XOR!

## Sigmoid function
![sigmoid function example](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/2880px-Logistic-curve.svg.png)
- Maps inputs to a range between 0 and 1
- Useful to represent probabilities
- Often used in output layers for binary classification.
- Not frequently used after hidden layers

> See the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) Wikipedia article.
## Tanh Function

- In practice, often better results than Sigmoid
- More commonly used as an activation function.
- Cannot be used in the output layer.

# ReLU Function

- Nearly linear
	- Much faster to compute than Sigmoid and Tanh
	- Faster training
- Very common in hidden layers
- Cannot be used in output layers

# A Simple Feedforward Network

![[simple feed forward.jpeg]]

## Multiple Classes
- What if you have more than two output classes?
- Add more output units (one for each class)
- If multiple classes are possible, multiple sigmoid functions
- Otherwise, use a softmax layer: $$\text{softmax}\left(\hat{y}_i \right) = \displaystyle \frac{\displaystyle e^{\hat{y}_i}}{\displaystyle\sum_{j=0}^N{e^{\hat{y}_j}}}$$
### Softmax
- Use for one-of-many predictions
- Gives you a probability distribution
	- Values are all between 0 and 1
	- Values add up to 1

## Activation functions

| Name                                             |                                                                     Function $g(x)$                                                                     |                                                                                               Derivative $$g'(x)$$                                                                                                |            Range             |                                     Order of continuity                                      | Advantages                                                                                                                                                                   | Disadvantages                                                                                                                                                                | Use Cases                                                                                                        |
| :----------------------------------------------- | :-----------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------: | :------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------- |
| **Identity**                                     |                                                                         $$ x $$                                                                         |                                                                                                      $$ 1 $$                                                                                                      |    $$(-\infty, \infty)$$     |                                         $$C^\infty$$                                         | Simple, no transformation. Allows output layer to produce values across any range.                                                                                           | Doesn't introduce non-linearity; stacking multiple linear layers is equivalent to a single linear layer.                                                                     | Output layer for Regression problems.                                                                            |
| **Binary step**                                  |                                      $$\begin{cases} 0 & \text{if } x < 0 \\ 1 & \text{if } x \ge 0 \end{cases} $$                                      |                                                                                                      $$ 0 $$                                                                                                      |         $$\{0, 1\}$$         |                                  $$C^{-1}$$ (Discontinuous)                                  | Extremely simple, clear thresholding.                                                                                                                                        | Zero derivative prevents gradient-based learning; discontinuous.                                                                                                             | Historically used in single-layer perceptrons; generally unsuitable for modern deep learning.                    |
| **Logistic, sigmoid, or soft step**              |                                                         $$ \sigma(x) = \frac{1}{1 + e^{-x}} $$                                                          |                                                                                     $$\sigma(x)'= \sigma(x)(1 - \sigma(x)) $$                                                                                     |          $$(0, 1)$$          |                                         $$C^\infty$$                                         | Smooth gradient, output interpretable as probability, bounded output.                                                                                                        | Vanishing gradient problem, output not zero-centered, computationally more expensive than ReLU variants.                                                                     | Output layer for Binary Classification. Older RNNs (less common now).                                            |
| **Hyperbolic tangent ($tanh$)**                  |                                                   $$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$                                                    |                                                                                               $$ 1 - \tanh(x)^2 $$                                                                                                |         $$(-1, 1)$$          |                                         $$C^\infty$$                                         | Zero-centered output (often helps convergence compared to sigmoid), smooth, bounded output.                                                                                  | Vanishing gradient problem (though often less severe than sigmoid), computationally more expensive than ReLU variants.                                                       | Hidden layers in Feedforward Networks and RNNs (often preferred over sigmoid).                                   |
| **Soboleva modified hyperbolic tangent ($smht)** |                                            $$ \text{smht}(x) = \frac{e^{ax} - e^{-bx}}{e^{cx} + e^{-dx}} $$                                             |                                           $$ \text{smht}(x)' = \frac{ae^{ax} + be^{-bx}}{e^{cx} + e^{-dx}} -\text{smht}(x)\frac{ce^{cx} - de^{-dx}}{e^{cx} + e^{-dx}}$$                                           |         $$(-1, 1)$$          |                                         $$C^\infty$$                                         | Potentially offers more flexibility in shape than standard tanh via parameters a, b, c, d.                                                                                   | Increased complexity due to parameters, potential for overfitting, unclear general benefits over simpler functions.                                                          | Research or specific tasks requiring fine-tuned activation shapes.                                               |
| **Softsign**                                     |                                                   $$\text{softsign}(x)=\frac{x}{1 +\lvert x \rvert}$$                                                   |                                                                             $$ \text{softsign}(x)'=\frac{1}{(1 +\lvert x \rvert)^2}$$                                                                             |          $$[-1,1]$$          |                                           $$C^1$$                                            | Smooth, bounded output, less prone to saturation than tanh/sigmoid (converges polynomially, not exponentially).                                                              | Gradient can still become very small, potentially slower convergence than ReLU.                                                                                              | Alternative to tanh in hidden layers, less commonly used.                                                        |
| **Rectified linear unit (ReLU)**                 |              $$ \displaystyle\left(x \right)^+ = \begin{cases} 0 & \text{if } x \le 0 \\ x & \text{if } x > 0 \end{cases}\ =\ \max(0, x)$$              |                                                                   $$ \begin{cases} 0 & \text{if } x < 0 \\ 1 & \text{if } x > 0 \end{cases} $$                                                                    |       $$[0, \infty)$$        |                                           $$C^0$$                                            | Computationally very efficient, avoids vanishing gradients for positive inputs, promotes sparsity.                                                                           | "Dying ReLU" problem (neurons can become permanently inactive), output is not zero-centered, zero gradient for negative inputs.                                              | Default and most popular choice for hidden layers in CNNs and Feedforward Networks.                              |
| **Gaussian Error Linear Unit (GELU)**            |                $$ \frac{1}{2} x \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right) = x \Phi(x) $$  ($\Phi$ is Gaussian CDF)[^1]                 |                                                                                        $$ \Phi(x) + x \phi(x) $$ See [^2]                                                                                         | $$(\approx -0.17, \infty)$$  |                                         $$C^\infty$$                                         | Smooth approximation of ReLU, non-monotonic, often performs well in practice, especially in Transformers.                                                                    | Computationally more expensive than ReLU due to the error function / CDF calculation.                                                                                        | Hidden layers in Transformer models (e.g., BERT, GPT variants).                                                  |
| **Softplus**                                     |                                                                   $$ \ln(1 + e^x) $$                                                                    |                                                                                       $$ \frac{1}{1 + e^{-x}} $$ (Sigmoid)                                                                                        |       $$(0, \infty)$$        |                                         $$C^\infty$$                                         | Smooth approximation of ReLU, differentiable everywhere, always positive output.                                                                                             | Output is not zero-centered, derivative never reaches zero (less sparse than ReLU), computationally more expensive.                                                          | Less common now; sometimes used where a smooth ReLU version is needed.                                           |
| **Exponential linear unit (ELU)**                |                    $$ \begin{cases} \alpha(e^x - 1) & \text{if } x \le 0 \\ x & \text{if } x > 0 \end{cases} $$ (parameter $\alpha$)                    |                                                               $$ \begin{cases} \alpha e^x & \text{if } x < 0 \\ 1 & \text{if } x > 0 \end{cases} $$                                                               |    $$(-\alpha, \infty)$$     | $$ \begin{cases} C^1 & \text{if } \alpha = 1 \\ C^0 & \text{if }\alpha \neq1  \end{cases} $$ | Mitigates the dying ReLU problem, resulting in output closer to zero-mean for negative inputs, and potentially achieving faster convergence than ReLU. Smooth if $\alpha=1$. | Computationally more expensive than ReLU (exponential function), introduces a hyperparameter $\alpha$.                                                                       | Alternative to ReLU in hidden layers, especially if dying ReLUs are a concern.                                   |
| **Scaled exponential linear unit (SELU)**        | $$ \lambda \begin{cases} \alpha(e^x - 1) & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases} $$ ($\lambda \approx 1.0507, \alpha \approx 1.67326$) |                                                          $$ \lambda \begin{cases} \alpha e^x & \text{if } x < 0 \\ 1 & \text{if } x \ge 0 \end{cases} $$                                                          | $$(-\lambda\alpha, \infty)$$ |                                           $$C^0$$                                            | Can induce self-normalizing properties in deep networks (activations converge towards zero mean, unit variance) under specific conditions.                                   | Requires specific weight initialization (`lecun_normal`) and network structure (alpha dropout); performance can be sensitive to these conditions. Computationally expensive. | Deep Feedforward Networks where self-normalization is desired.                                                   |
| **Leaky rectified linear unit (Leaky ReLU)**     |                                   $$ \begin{cases} 0.01x & \text{if } x \le 0 \\ x & \text{if } x > 0 \end{cases} $$                                    |                                                                  $$ \begin{cases} 0.01 & \text{if } x < 0 \\ 1 & \text{if } x > 0 \end{cases} $$                                                                  |    $$(-\infty, \infty)$$     |                                           $$C^0$$                                            | Addresses dying ReLU problem by allowing a small gradient for negative inputs, computationally efficient.                                                                    | Performance improvement over ReLU is not always guaranteed, and it introduces a small hyperparameter (the leak slope, often fixed at 0.01).                                  | Common alternative to ReLU, particularly if dying neurons are suspected.                                         |
| **Parametric rectified linear unit (PReLU)**     |                       $$ \begin{cases} \alpha x & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases} $$ (parameter $\alpha$)                        |                                                                $$ \begin{cases} \alpha & \text{if } x < 0 \\ 1 & \text{if } x \ge 0 \end{cases} $$                                                                |    $$(-\infty, \infty)$$     |                                           $$C^0$$                                            | Allows the network to learn the optimal leak slope $\alpha$$ per channel or layer, potentially better performance than Leaky ReLU.                                           | Adds parameters to the model (risk of overfitting), slightly increases model complexity.                                                                                     | Alternative to ReLU/Leaky ReLU where learning the negative slope might be beneficial.                            |
| **Rectified Parametric Sigmoid Units (RPSU)**    |            $$ \alpha(2x 1_{\{x \ge \lambda\}} - g_{\lambda,\sigma,\mu,\beta}(x)) + (1-\alpha)g_{\lambda,\sigma,\mu,\beta}(x) $$ where $$a$$             |                                                                                                                                                                                                                   |    $$(-\infty, \infty)$$     |                                           $$C^0$$                                            | Highly flexible due to multiple parameters, allowing complex activation shapes.                                                                                              | Extremely complex, with many parameters that make it hard to tune, high risk of overfitting, and interpretability is low.                                                    | Primarily for research exploring highly parameterized activation functions.                                      |
| **Sigmoid linear unit (SiLU / Swish-1)**         |                                                               $$ \frac{x}{1 + e^{-x}} $$                                                                |                                                                                $$ \frac{1 + e^{-x} + x e^{-x}}{(1 + e^{-x})^2} $$                                                                                 | $$[\approx -0.278, \infty)$$ |                                         $$C^\infty$$                                         | Smooth, non-monotonic, often outperforms ReLU in deeper models, self-gating property.                                                                                        | Computationally more expensive than ReLU.                                                                                                                                    | Increasingly popular alternative to ReLU in modern architectures (e.g., EfficientNet, some vision transformers). |
| **Exponential Linear Sigmoid Squashing (ELiSH)** |               $$ \begin{cases} \frac{e^x - 1}{1 + e^{-x}} & \text{if } x < 0 \\ \frac{x}{1 + e^{-x}} & \text{if } x \ge 0 \end{cases} $$                | $$ \begin{cases} \frac{e^x(1+e^{-x}) + (e^x-1)e^{-x}}{(1+e^{-x})^2} & \text{if } x < 0 \\ \frac{(1+e^{-x}) + x e^{-x}}{(1+e^{-x})^2} & \text{if } x \ge 0 \end{cases} $$ (Simplified derivatives shown in source) | $$[\approx -0.881, \infty)$$ |                                           $$C^1$$                                            | Smooth ($$C^1$$), non-monotonic, combines properties of ELU and SiLU/Swish.                                                                                                  | Increased computational complexity compared to ReLU. Benefits over simpler functions like Swish or GELU may not always be significant.                                       | Research, potential alternative to Swish/GELU in various deep learning models.                                   |
| **Gaussian**                                     |                                                                     $$ e^{-x^2} $$                                                                      |                                                                                                $$ -2x e^{-x^2} $$                                                                                                 |          $$(0, 1]$$          |                                         $$C^\infty$$                                         | Smooth, localized response (activates strongly only for inputs near zero), bounded output.                                                                                   | Not monotonic, gradients vanish quickly away from zero, output always positive.                                                                                              | Radial Basis Function (RBF) networks, specific niche applications.                                               |
| **Sinusoid (sin(x))**                            |                                                                 $$ \sin(x) $$ (Assumed)                                                                 |                                                                                              $$ \cos(x) $$ (Assumed)                                                                                              |         $$[-1, 1]$$          |                                         $$C^\infty$$                                         | Introduces periodicity, smooth, bounded. Useful for representing signals or periodic patterns.                                                                               | Periodicity might be undesirable for many tasks, gradient oscillates and can vanish.                                                                                         | Representing periodic functions, Implicit Neural Representations (e.g., NeRF), signal processing tasks.          |

> [^1]: $\text{erf}(z)=\displaystyle \frac2{\sqrt{\pi}} \displaystyle \int_{0}^{z} e^{-t^2}dt$ [Gauss error function](https://en.wikipedia.org/wiki/Error_function)
> [^2]: $\phi(x) =\displaystyle \frac1{\sqrt{2\pi}}e^{-t^2}$ Probability density function of standard gaussian distribution.



## Resources:

https://paperswithcode.com/method/gelu
