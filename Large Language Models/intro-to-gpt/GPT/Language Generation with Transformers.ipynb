{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:00.934447Z",
     "start_time": "2025-05-09T03:01:58.105552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch # We'll load Pytorch so we can convert a list to a tensor\n",
    "from scipy.special import softmax\n",
    "import numpy as np # We're using numpy to use its argmax function\n",
    "import random\n",
    "from transformers import pipeline"
   ],
   "id": "3e27a4614d400caf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Language Generation with Transformers\n",
    "\n",
    "When predicting the next token, a GPT model can give us a score for all possible next tokens. We can use those probabilities to generate new text, potentially by selecting the most likely next token or by sampling using the probabilities. Let's see how that works.\n",
    "\n",
    "Let's say that we want to generate more text after the sequence below:"
   ],
   "id": "804764ea28875540"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.060912Z",
     "start_time": "2025-05-09T03:02:01.059333Z"
    }
   },
   "source": "text = 'The quick brown fox jumped over'",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll need to load the tokenizer and model for `distilgpt2`.",
   "id": "1efbf39fefdcd23e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.741538Z",
     "start_time": "2025-05-09T03:02:01.068879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ],
   "id": "821e812f79b6313d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As before, we use the tokenizer to tokenize the text and convert each token to its token ID. We will use the `.encode` function to get the token IDs back as a Python list as they are easier to manipulate. We'll want to add extra token IDs that we've generated!",
   "id": "7100bfed669615c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.768310Z",
     "start_time": "2025-05-09T03:02:01.762737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.encode(text)\n",
    "input_ids"
   ],
   "id": "6fa446b01f5e275b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 2068, 7586, 21831, 11687, 625]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use the `tokenizer.decode` function to turn the token IDs back into text. This will be useful after we've generated further token IDs to add on the end",
   "id": "c5defa1f268897ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.782941Z",
     "start_time": "2025-05-09T03:02:01.780331Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(input_ids)",
   "id": "5d54ac020360c759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's run the token IDs through the `distilgpt2` model and get the probabilities of the next token",
   "id": "4b04f1a748a2eb8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.837440Z",
     "start_time": "2025-05-09T03:02:01.797337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "as_tensor = torch.tensor(input_ids).reshape(1,-1) # This converts the token ID list to a tensor\n",
    "output = model(input_ids=as_tensor) # We pass it into the model\n",
    "next_token_scores = output.logits[0,-1,:].detach().numpy() # We get the scores for next token and the end of the sequence (token index=-1)\n",
    "next_token_probs = softmax(next_token_scores) # And we apply a softmax function\n",
    "\n",
    "next_token_probs.shape"
   ],
   "id": "6b2dd8b973da8096",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we've got the probabilities for all possible 50257 tokens to be after our input text sequence.\n",
    "\n",
    "Let's get the one with the highest probability. For that we can use the `argmax` function."
   ],
   "id": "cd4404beb0296aaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.861904Z",
     "start_time": "2025-05-09T03:02:01.858971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_id = next_token_probs.argmax()\n",
    "next_token_id"
   ],
   "id": "1e65d9c7f2cbace5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(262)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hmm, the token with ID=262 has the highest probability. But what token is that? `tokenizer.decode` can tell us:",
   "id": "d737b4e701ae9a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:01.884660Z",
     "start_time": "2025-05-09T03:02:01.882501Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(next_token_id)",
   "id": "e1949302a6d5ae8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we've all the parts we need. Your task is to calculate the next eight tokens after `input_ids` (including the one we calculated above). You'll be adding `1353` to the input token IDs, running it through the model again and deciding the next token. Try writing it as a loop that iterates eight times.",
   "id": "75555359f12dcdb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.124976Z",
     "start_time": "2025-05-09T03:02:01.908683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def probability_generator(input_ids_, model_):\n",
    "    output_ = model_(input_ids=input_ids_)\n",
    "    next_token_scores_ = output_.logits[0,-1,:].detach().numpy() # We get the scores for next token and the end of the sequence (token index=-1)\n",
    "    return softmax(next_token_scores_) # And we apply a softmax function\n",
    "\n",
    "def greedy_ids_generator(input_ids_, model_, n=8):\n",
    "    for i in range(n):\n",
    "        next_token_probs_ = probability_generator(input_ids_, model_)\n",
    "        next_token_id_ = next_token_probs_.argmax() # Get the token ID with the highest probability\n",
    "        input_ids_ = torch.cat((input_ids_, torch.tensor([next_token_id_]).reshape(1,-1)), dim=1) # Add the new token ID to the input IDs\n",
    "    return input_ids_\n",
    "\n",
    "tensor_modified = torch.cat((as_tensor, torch.tensor([1353]).reshape(1,-1)), dim=1)\n",
    "greedy_generated = greedy_ids_generator(as_tensor, model)\n",
    "greedy_generated_modified = greedy_ids_generator(tensor_modified, model)"
   ],
   "id": "a13d14d12027a1e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.144173Z",
     "start_time": "2025-05-09T03:02:02.142285Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=greedy_generated[0, :].tolist())",
   "id": "fa145a71ab517ece",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over the fence and ran over the fence.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.163550Z",
     "start_time": "2025-05-09T03:02:02.160912Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=greedy_generated_modified[0, :].tolist())",
   "id": "b36d83ad9879f803",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over top of the fox and then jumped over the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With eight extra tokens, you should get a list with IDs = `[464, 2068, 7586, 21831, 11687, 625, 262, 13990, 290, 4966, 625, 262, 13990, 13]` which decodes to give the text: \"The quick brown fox jumped over the fence and ran over the fence.\".",
   "id": "f2f898952ff676ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now picking the token with highest probability every time can often create quite boring text. Sampling from the tokens can generate more interesting text. Sampling uses the probabilities as weights so that words with higher probabilities are more likely to be chosen. Let's see how that works:\n",
    "\n",
    "Let's imagine we've got a probabilities for four possible tokens (a very tiny vocabulary)."
   ],
   "id": "ee5832cd42b6534b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.185315Z",
     "start_time": "2025-05-09T03:02:02.183566Z"
    }
   },
   "cell_type": "code",
   "source": "next_token_probs = np.array([0.1, 0.2, 0.5, 0.3])",
   "id": "22a6dcb34a467456",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we saw above, we can use `argmax` that tells us the index of the highest value. In this case, it's index=2",
   "id": "df392d731e3c856a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.204975Z",
     "start_time": "2025-05-09T03:02:02.202824Z"
    }
   },
   "cell_type": "code",
   "source": "next_token_probs.argmax()",
   "id": "c109b809763f8834",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, let's say we want to sample randomly from the possible token indices (`[0, 1, 2, 3]`). First, let's create that list to sample from:",
   "id": "aad996b5f9bb3389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.224458Z",
     "start_time": "2025-05-09T03:02:02.222224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indices = list(range(len(next_token_probs)))\n",
    "indices"
   ],
   "id": "a5107f0f0400fcc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We could use the [choices](https://docs.python.org/3/library/random.html#random.choices) function to pick a single token ID with all four being equally likely to be chosen",
   "id": "db4a8c9e3bc64502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.245203Z",
     "start_time": "2025-05-09T03:02:02.242788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_id = random.choices(indices, k=1)[0]\n",
    "next_token_id"
   ],
   "id": "4c22148e1a4dbb30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or we could provide weights, such that some of the tokens are more likely to be chosen than others. In this case, we provide `next_token_probs` as weights.",
   "id": "5670d99e1eafa24d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.270876Z",
     "start_time": "2025-05-09T03:02:02.268483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_id = random.choices(indices, k=1, weights=next_token_probs)[0]\n",
    "next_token_id"
   ],
   "id": "5fed9ff1a6fd51a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That would allow us to sample from the token probability distribution.\n",
    "\n",
    "Your task is to generate some new text (starting from \"The quick brown fox jumped over\" as before) using sampling and the `random.choices` function to pick your next token. Try it with weighting and without weighting to see what happens."
   ],
   "id": "3e96f29b26f8fc82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.532525Z",
     "start_time": "2025-05-09T03:02:02.288986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sampling_generator(input_ids_, model_, n=8):\n",
    "    for i in range(n):\n",
    "        next_token_probs_ = probability_generator(input_ids_, model_)\n",
    "        indices_ = list(range(len(next_token_probs_)))\n",
    "        next_token_id_ = random.choices(indices_, k=1, weights=next_token_probs_)[0] # Get the token ID with the highest probability\n",
    "        input_ids_ = torch.cat((input_ids_, torch.tensor([next_token_id_]).reshape(1,-1)), dim=1) # Add the new token ID to the input IDs\n",
    "    return input_ids_\n",
    "\n",
    "sampling_generated = sampling_generator(as_tensor, model)\n",
    "sampling_generated_modified = sampling_generator(tensor_modified, model)"
   ],
   "id": "e6b626c40e4963ae",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.551583Z",
     "start_time": "2025-05-09T03:02:02.549425Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=sampling_generated[0, :].tolist())",
   "id": "1976f7ecf250c421",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over and over and over again after the attack'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T03:02:02.580118Z",
     "start_time": "2025-05-09T03:02:02.577766Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=sampling_generated_modified[0, :].tolist())",
   "id": "304c844bddf941af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over top and whispered loudly, \\u202a,�'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Try running your code again and you should get a different output due to the random nature of the sampling. There's a lot of tweaks that can be made to the random sampling strategy.\n",
    "\n",
    "Fortunately, we don't have to implement all the different text generation functions ourselves. The HuggingFace library provides a `text-generation` pipeline to generate text.\n",
    "\n",
    "For example, here is how to run it and request 30 extra tokens and 5 different generations."
   ],
   "id": "ec468293580fb5f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f378eb4e083e0938"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
