{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:54.528682Z",
     "start_time": "2025-05-09T02:42:51.580572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch # We'll load Pytorch so we can convert a list to a tensor\n",
    "from scipy.special import softmax\n",
    "import numpy as np # We're using numpy to use its argmax function\n",
    "import random\n",
    "from transformers import pipeline"
   ],
   "id": "3e27a4614d400caf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Language Generation with Transformers\n",
    "\n",
    "When predicting the next token, a GPT model can give us a score for all possible next tokens. We can use those probabilities to generate new text, potentially by selecting the most likely next token or by sampling using the probabilities. Let's see how that works.\n",
    "\n",
    "Let's say that we want to generate more text after the sequence below:"
   ],
   "id": "804764ea28875540"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:54.640476Z",
     "start_time": "2025-05-09T02:42:54.639018Z"
    }
   },
   "source": "text = 'The quick brown fox jumped over'",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll need to load the tokenizer and model for `distilgpt2`.",
   "id": "1efbf39fefdcd23e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:55.608302Z",
     "start_time": "2025-05-09T02:42:54.659042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ],
   "id": "821e812f79b6313d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As before, we use the tokenizer to tokenize the text and convert each token to its token ID. We will use the `.encode` function to get the token IDs back as a Python list as they are easier to manipulate. We'll want to add extra token IDs that we've generated!",
   "id": "7100bfed669615c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:55.632917Z",
     "start_time": "2025-05-09T02:42:55.626911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.encode(text)\n",
    "input_ids"
   ],
   "id": "6fa446b01f5e275b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 2068, 7586, 21831, 11687, 625]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use the `tokenizer.decode` function to turn the token IDs back into text. This will be useful after we've generated further token IDs to add on the end",
   "id": "c5defa1f268897ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:55.657915Z",
     "start_time": "2025-05-09T02:42:55.655797Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(input_ids)",
   "id": "5d54ac020360c759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's run the token IDs through the `distilgpt2` model and get the probabilities of the next token",
   "id": "4b04f1a748a2eb8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:56.218346Z",
     "start_time": "2025-05-09T02:42:55.677073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "as_tensor = torch.tensor(input_ids).reshape(1,-1) # This converts the token ID list to a tensor\n",
    "output = model(input_ids=as_tensor) # We pass it into the model\n",
    "next_token_scores = output.logits[0,-1,:].detach().numpy() # We get the scores for next token and the end of the sequence (token index=-1)\n",
    "next_token_probs = softmax(next_token_scores) # And we apply a softmax function\n",
    "\n",
    "next_token_probs.shape"
   ],
   "id": "6b2dd8b973da8096",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we've got the probabilities for all possible 50257 tokens to be after our input text sequence.\n",
    "\n",
    "Let's get the one with the highest probability. For that we can use the `argmax` function."
   ],
   "id": "cd4404beb0296aaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:56.240600Z",
     "start_time": "2025-05-09T02:42:56.238472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_id = next_token_probs.argmax()\n",
    "next_token_id"
   ],
   "id": "1e65d9c7f2cbace5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(262)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hmm, the token with ID=262 has the highest probability. But what token is that? `tokenizer.decode` can tell us:",
   "id": "d737b4e701ae9a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:42:56.261110Z",
     "start_time": "2025-05-09T02:42:56.258920Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(next_token_id)",
   "id": "e1949302a6d5ae8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we've all the parts we need. Your task is to calculate the next eight tokens after `input_ids` (including the one we calculated above). You'll be adding `1353` to the input token IDs, running it through the model again and deciding the next token. Try writing it as a loop that iterates eight times.",
   "id": "75555359f12dcdb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:45:47.570414Z",
     "start_time": "2025-05-09T02:45:47.301912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def greedy_ids_generator(input_ids_, model_, n=8):\n",
    "    for i in range(n):\n",
    "        output_ = model_(input_ids=input_ids_)\n",
    "        next_token_scores_ = output_.logits[0,-1,:].detach().numpy() # We get the scores for next token and the end of the sequence (token index=-1)\n",
    "        next_token_probs_ = softmax(next_token_scores_) # And we apply a softmax function\n",
    "        next_token_id_ = next_token_probs_.argmax() # Get the token ID with the highest probability\n",
    "        input_ids_ = torch.cat((input_ids_, torch.tensor([next_token_id_]).reshape(1,-1)), dim=1) # Add the new token ID to the input IDs\n",
    "    return input_ids_\n",
    "\n",
    "tensor_modified = torch.cat((as_tensor, torch.tensor([1353]).reshape(1,-1)), dim=1)\n",
    "greedy_generated = greedy_ids_generator(as_tensor, model)\n",
    "greedy_generated_modified = greedy_ids_generator(tensor_modified, model)"
   ],
   "id": "a13d14d12027a1e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:45:49.104815Z",
     "start_time": "2025-05-09T02:45:49.101435Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=greedy_generated[0, :].tolist())",
   "id": "fa145a71ab517ece",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over the fence and ran over the fence.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:45:50.436264Z",
     "start_time": "2025-05-09T02:45:50.433077Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_ids=greedy_generated_modified[0, :].tolist())",
   "id": "b36d83ad9879f803",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over top of the fox and then jumped over the'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
