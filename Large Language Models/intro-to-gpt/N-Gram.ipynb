{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# N-Gram\n",
    "\n",
    "We'll assume that our text is already \"tokenized\" (split up into words). We'll cover this process in more depth in the next module.\n",
    "\n",
    "As an example, let's work with two sentences from \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", a short story written by [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ],
   "id": "65cc083708269d2c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T01:14:03.461802Z",
     "start_time": "2025-04-30T01:14:03.440404Z"
    }
   },
   "source": [
    "# Tokens for the sentence \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astute and dangerous man.\"\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens for the sentence \"How would Lausanne do, my dear Watson?\"\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Your first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ],
   "id": "b60cbec62f98c385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:15:41.913862Z",
     "start_time": "2025-04-30T01:15:41.909064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    Build n-grams from a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens.\n",
    "        n (int): The size of the n-grams.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str]]: List of n-grams as tuples.\n",
    "    \"\"\"\n",
    "    return [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "\n",
    "# Example:\n",
    "build_ngrams(sample1, n=2)"
   ],
   "id": "5f3cf297a6c6a252",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'shows'),\n",
       " ('shows', ','),\n",
       " (',', 'my'),\n",
       " ('my', 'dear'),\n",
       " ('dear', 'Watson'),\n",
       " ('Watson', ','),\n",
       " (',', 'that'),\n",
       " ('that', 'we'),\n",
       " ('we', 'are'),\n",
       " ('are', 'dealing'),\n",
       " ('dealing', 'with'),\n",
       " ('with', 'an'),\n",
       " ('an', 'exceptionally'),\n",
       " ('exceptionally', 'astute'),\n",
       " ('astute', 'and'),\n",
       " ('and', 'dangerous'),\n",
       " ('dangerous', 'man'),\n",
       " ('man', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:16:19.517280Z",
     "start_time": "2025-04-30T01:16:19.514132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams(sample1, n=3)) == 17\n",
    "assert build_ngrams(sample1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert build_ngrams(sample1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(build_ngrams(sample1, n=2)) == 18\n",
    "assert build_ngrams(sample1, n=2)[0] == ('It', 'shows')\n",
    "assert build_ngrams(sample1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(build_ngrams(sample2, n=2)) == 8\n",
    "assert build_ngrams(sample2, n=2)[0] == ('How', 'would')"
   ],
   "id": "c0f913ebc3c243b2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of your `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ],
   "id": "640f349d466cd169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:23:02.238875Z",
     "start_time": "2025-04-30T01:23:02.229882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    Build n-grams from a list of tokens, including control tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens.\n",
    "        n (int): The size of the n-grams.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str]]: List of n-grams as tuples with control tokens.\n",
    "    \"\"\"\n",
    "    # Add control tokens\n",
    "    tokens = [BOS] * (n - 1) + tokens + [EOS] * (n - 1)\n",
    "    return build_ngrams(tokens, n)\n",
    "\n",
    "\n",
    "# Example:\n",
    "build_ngrams_ctrl(sample1, n=3)"
   ],
   "id": "5b9d1ffa91ba730c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', '<BOS>', 'It'),\n",
       " ('<BOS>', 'It', 'shows'),\n",
       " ('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.'),\n",
       " ('man', '.', '<EOS>'),\n",
       " ('.', '<EOS>', '<EOS>')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:23:05.006199Z",
     "start_time": "2025-04-30T01:23:05.002793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams_ctrl(sample1, n=3)) == 21\n",
    "assert build_ngrams_ctrl(sample1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample1, n=2)) == 20\n",
    "assert build_ngrams_ctrl(sample1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample2, n=2)) == 10\n",
    "assert build_ngrams_ctrl(sample2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert build_ngrams_ctrl(sample2, n=2)[9] == ('?', '<EOS>')"
   ],
   "id": "a83007cbdb9cd749",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ],
   "id": "79f0c0bfc53b69d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:37:04.797864Z",
     "start_time": "2025-04-30T01:37:04.791836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Count n-grams from a list of tokenized texts.\n",
    "\n",
    "    Args:\n",
    "        texts (List[List[str]]): List of tokenized texts.\n",
    "        n (int): The size of the n-grams.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str, ...], Dict[str, int]]: Dictionary mapping n-grams to their counts.\n",
    "    \"\"\"\n",
    "    ngram_counts = {}\n",
    "\n",
    "    for tokens in texts:\n",
    "        ngrams = build_ngrams_ctrl(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            last_word = ngram[-1]\n",
    "            key = ngram[:-1]\n",
    "            ngram_count = ngram_counts.get(key, {})\n",
    "            ngram_count[last_word] = ngram_count.get(last_word, 0) + 1\n",
    "            ngram_counts[key] = ngram_count\n",
    "\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ],
   "id": "bd16cc5a2d2b5f2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
       " ('<BOS>', 'It'): {'shows': 1},\n",
       " ('It', 'shows'): {',': 1},\n",
       " ('shows', ','): {'my': 1},\n",
       " (',', 'my'): {'dear': 2},\n",
       " ('my', 'dear'): {'Watson': 2},\n",
       " ('dear', 'Watson'): {',': 1, '?': 1},\n",
       " ('Watson', ','): {'that': 1},\n",
       " (',', 'that'): {'we': 1},\n",
       " ('that', 'we'): {'are': 1},\n",
       " ('we', 'are'): {'dealing': 1},\n",
       " ('are', 'dealing'): {'with': 1},\n",
       " ('dealing', 'with'): {'an': 1},\n",
       " ('with', 'an'): {'exceptionally': 1},\n",
       " ('an', 'exceptionally'): {'astute': 1},\n",
       " ('exceptionally', 'astute'): {'and': 1},\n",
       " ('astute', 'and'): {'dangerous': 1},\n",
       " ('and', 'dangerous'): {'man': 1},\n",
       " ('dangerous', 'man'): {'.': 1},\n",
       " ('man', '.'): {'<EOS>': 1},\n",
       " ('.', '<EOS>'): {'<EOS>': 1},\n",
       " ('<BOS>', 'How'): {'would': 1},\n",
       " ('How', 'would'): {'Lausanne': 1},\n",
       " ('would', 'Lausanne'): {'do': 1},\n",
       " ('Lausanne', 'do'): {',': 1},\n",
       " ('do', ','): {'my': 1},\n",
       " ('Watson', '?'): {'<EOS>': 1},\n",
       " ('?', '<EOS>'): {'<EOS>': 1}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:37:28.448095Z",
     "start_time": "2025-04-30T01:37:28.444421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tests:\n",
    "assert len(count_ngrams([sample1, sample2], n=3)) == 28\n",
    "assert len(count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(count_ngrams([sample1, sample2], n=2)) == 24\n",
    "assert len(count_ngrams([sample1, sample2], n=2)['<BOS>',]) == 2\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['dear',]['Watson'] == 2"
   ],
   "id": "69e877a7c89812c6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ],
   "id": "6f71e714decb0027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:42:07.910516Z",
     "start_time": "2025-04-30T01:42:07.905823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Build an n-gram language model from a list of tokenized texts.\n",
    "\n",
    "    Args:\n",
    "        texts (List[List[str]]): List of tokenized texts.\n",
    "        n (int): The size of the n-grams.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str, ...], Dict[str, float]]: A dictionary where keys are n-grams of size n-1,\n",
    "        and values are dictionaries mapping the next word to its probability.\n",
    "    \"\"\"\n",
    "\n",
    "    ngram_counts = count_ngrams(texts, n)\n",
    "    for key in ngram_counts.keys():\n",
    "        total_count = sum(ngram_counts[key].values())\n",
    "        for word in ngram_counts[key]:\n",
    "            ngram_counts[key][word] /= total_count\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ],
   "id": "9ef9272b3fdd85f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       " ('<BOS>', 'It'): {'shows': 1.0},\n",
       " ('It', 'shows'): {',': 1.0},\n",
       " ('shows', ','): {'my': 1.0},\n",
       " (',', 'my'): {'dear': 1.0},\n",
       " ('my', 'dear'): {'Watson': 1.0},\n",
       " ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       " ('Watson', ','): {'that': 1.0},\n",
       " (',', 'that'): {'we': 1.0},\n",
       " ('that', 'we'): {'are': 1.0},\n",
       " ('we', 'are'): {'dealing': 1.0},\n",
       " ('are', 'dealing'): {'with': 1.0},\n",
       " ('dealing', 'with'): {'an': 1.0},\n",
       " ('with', 'an'): {'exceptionally': 1.0},\n",
       " ('an', 'exceptionally'): {'astute': 1.0},\n",
       " ('exceptionally', 'astute'): {'and': 1.0},\n",
       " ('astute', 'and'): {'dangerous': 1.0},\n",
       " ('and', 'dangerous'): {'man': 1.0},\n",
       " ('dangerous', 'man'): {'.': 1.0},\n",
       " ('man', '.'): {'<EOS>': 1.0},\n",
       " ('.', '<EOS>'): {'<EOS>': 1.0},\n",
       " ('<BOS>', 'How'): {'would': 1.0},\n",
       " ('How', 'would'): {'Lausanne': 1.0},\n",
       " ('would', 'Lausanne'): {'do': 1.0},\n",
       " ('Lausanne', 'do'): {',': 1.0},\n",
       " ('do', ','): {'my': 1.0},\n",
       " ('Watson', '?'): {'<EOS>': 1.0},\n",
       " ('?', '<EOS>'): {'<EOS>': 1.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:42:28.230984Z",
     "start_time": "2025-04-30T01:42:28.227834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tests:\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['dear',]['Watson'] == 1.0"
   ],
   "id": "c69ac4a4c482d17d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!",
   "id": "3568cec987762e73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:45:43.451244Z",
     "start_time": "2025-04-30T01:45:43.061425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "model = build_ngram_model(full_text, n=3)"
   ],
   "id": "720adf910967670f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T01:46:06.053593Z",
     "start_time": "2025-04-30T01:46:06.049099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs) - 5} more...]')"
   ],
   "id": "8e6a1a1a1c81a9c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n",
      "\t\"\t0.8073\n",
      "\tThe\t0.0304\n",
      "\tHolmes\t0.0230\n",
      "\tI\t0.0167\n",
      "\tIt\t0.0141\n",
      "\t[206 more...]\n",
      "<BOS> It\n",
      "\twas\t0.8235\n",
      "\tis\t0.0515\n",
      "\tmay\t0.0221\n",
      "\thad\t0.0147\n",
      "\tseemed\t0.0074\n",
      "\t[11 more...]\n",
      "It was\n",
      "\ta\t0.1888\n",
      "\tthe\t0.0686\n",
      "\tnot\t0.0562\n",
      "\tonly\t0.0312\n",
      "\tan\t0.0250\n",
      "\t[184 more...]\n",
      "my dear\n",
      "\tWatson\t0.5612\n",
      "\tfellow\t0.1429\n",
      "\tsir\t0.0918\n",
      "\tyoung\t0.0510\n",
      "\tVon\t0.0204\n",
      "\t[12 more...]\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
