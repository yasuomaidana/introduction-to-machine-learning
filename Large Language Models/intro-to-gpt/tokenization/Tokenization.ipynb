{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "Splitting a text block into meaningful subunits is essential to processing text. Text could be split into individual characters, words, or somewhere in between. A very basic approach is shown below that splits up text using white space. There's already a shortcoming, as the final word, 'dog,' has punctuation attached to it."
   ],
   "id": "cfa80426ba02ce25"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:26.235594Z",
     "start_time": "2025-05-05T01:11:26.233132Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "'The quick brown fox jumps over the lazy dog.'.split(' ')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With Transformer models, we do subword tokenization and split the text up using a prebuilt tokenizer. This has been trained on a large amount of text where it has learned what are common words and which are less common and could be split into parts (that often look like syllables).\n",
    "\n",
    "First let's load one for a common Transformer model `distilgpt2`. We can load it with the code below. The `distilgpt2` model is a smaller model based upon `gpt2` which is a predecessor to the language model that underpins ChatGPT.\n",
    "\n",
    "> To use the code below, you need to install the `transformers` library. "
   ],
   "id": "865be65239d770c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> To get rid of the warning, install `torch` or `tensorflow` and `ipywidgets`",
   "id": "b086158c7252e57e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.148641Z",
     "start_time": "2025-05-05T01:11:26.335310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ],
   "id": "443e4aca5ab7cfa0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The tokenizer has a function `tokenizer.tokenize` that splits up text.",
   "id": "77e34f7150aa1d20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.165392Z",
     "start_time": "2025-05-05T01:11:28.162319Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.tokenize(\"The quick fox jumps over the dog.\")",
   "id": "8d250ff95f937ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ġquick', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġdog', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.177707Z",
     "start_time": "2025-05-05T01:11:28.174907Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.tokenize(\"I visited Glasgow.\")",
   "id": "50642ace484c6f47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'Ġvisited', 'ĠGlasgow', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You should four tokens, with some starting with an odd character 'Ġ'. That 'Ġ' denotes that the token starts a new word. Try tokenizing \"volcano\" below with `tokenizer.tokenize` again. It should be split up into two subword tokens.",
   "id": "204baa2d0c0ad75b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.189815Z",
     "start_time": "2025-05-05T01:11:28.187477Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.tokenize(\"volcano\")",
   "id": "b6ee3df7a909f71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vol', 'cano']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Along with tokenizing the text into tokens/subtokens, we actually want the tokens to be mapped to numbers. The Transformers take the token indices as input. For example, the token index for the word 'Glasgow' is.",
   "id": "87c8d43f6eb623a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.216986Z",
     "start_time": "2025-05-05T01:11:28.205387Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.vocab['ĠGlasgow']",
   "id": "5593efdd152a1eeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`tokenizer.vocab` is a big dictionary mapping subword tokens to their indices. Let's see how big the vocabulary that the `distilgpt2` tokenizer has:",
   "id": "5ad282c9bf3e9ca8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.240052Z",
     "start_time": "2025-05-05T01:11:28.229057Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenizer.vocab)",
   "id": "50f26ef4305d5a82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We could manually map the tokenized output to the token indices.But the tokenizer can do it for us using `tokenizer.encode`.",
   "id": "8cb0e671399f0ee7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.247281Z",
     "start_time": "2025-05-05T01:11:28.244997Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.encode(\"I visited Glasgow.\")",
   "id": "1f9ff15a0d5a08bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 8672, 23995, 13]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can use the `tokenizer.decode` function to convert from a list of indices back to text.",
   "id": "28301602fe35bce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.261639Z",
     "start_time": "2025-05-05T01:11:28.259799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [[40, 8672, 23995, 13],[464, 7850, 46922, 4539, 832, 23995, 13]]\n",
    "for sentence in sentences:\n",
    "    print(tokenizer.decode(sentence))"
   ],
   "id": "e4dcdac1916a8ac9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited Glasgow.\n",
      "The river Clyde runs through Glasgow.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The tokenizer has a lot of parameters to give extra control. For instance, you sometimes need to truncate very long strings (as there is a limit on the length of input to Transformer models). Use the `tokenizer.encode` function to tokenize \"Kelvingrove is a beautiful park in Glasgow.\" and also trim it to only 5 tokens using `truncation=True` and `max_length=5`.",
   "id": "6aa7a9e2d5152c95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.276482Z",
     "start_time": "2025-05-05T01:11:28.274072Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.encode(\"Kelvingrove is a beautiful park in Glasgow.\", truncation=True, max_length=5)",
   "id": "91841340f70c2a08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 417, 1075, 305, 303]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the most common way to use a tokenizer is below which outputs a format ready to pass into a Transformer model. It uses `return_tensors='pt'` which tells it to return PyTorch tensors. PyTorch tensors are a data structure used for deep learning.\n",
    "\n",
    "The output has the `input_ids` which are the token indices as well as an `attention_mask` which can be used to tell a Transformer to ignore certain tokens. This occurs when using padding to deal with some sequences being shorter than others. That's not the case here, so the attention values are all one."
   ],
   "id": "4cc4d95f8fb1eed5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.294601Z",
     "start_time": "2025-05-05T01:11:28.289393Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer('Kelvingrove is a park in Glasgow.', return_tensors='pt')",
   "id": "5cd0496677c3da45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   42,   417,  1075,   305,   303,   318,   257,  3952,   287, 23995,\n",
       "            13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It should be noted that each tokenizer is very specific to the text it was trained on. For instance, below is a tokenizer that was trained on Spanish text.",
   "id": "aaca9971a9b95739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.587248Z",
     "start_time": "2025-05-05T01:11:28.306353Z"
    }
   },
   "cell_type": "code",
   "source": "spanish_tokenizer = AutoTokenizer.from_pretrained('datificate/gpt2-small-spanish')",
   "id": "8537b4bdca9e6753",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we give it a previous sentence in English, it tokenizes it very differently and splits up common English words into multiple parts.",
   "id": "423bac778c68d801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.601260Z",
     "start_time": "2025-05-05T01:11:28.598805Z"
    }
   },
   "cell_type": "code",
   "source": "spanish_tokenizer.tokenize('The river Clyde runs through Glasgow.')",
   "id": "1dcf7ec2d1b78f9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ġri', 'ver', 'ĠClyde', 'Ġr', 'uns', 'Ġth', 'rough', 'ĠGlasgow', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, it will tokenize Spanish effectively:",
   "id": "56f0ae0e67d90e98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.621106Z",
     "start_time": "2025-05-05T01:11:28.618517Z"
    }
   },
   "cell_type": "code",
   "source": "spanish_tokenizer.tokenize('Que te vaya bien')",
   "id": "ffa51fcb4fa87fcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Que', 'Ġte', 'Ġvaya', 'Ġbien']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sub-tokenization examples",
   "id": "80369c4f7efeb111"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.634672Z",
     "start_time": "2025-05-05T01:11:28.632655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [\"unhappiness\",\"generalization\",\"understand\",\"caregiver\",\"understandable\",\n",
    "         \"counterintuitive\",\"uncharacteristic\",\"misunderstanding\",\"disestablishmentarianism\",\"antidisestablishmentarianism\"]"
   ],
   "id": "d9c1cfcfd360900f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.648280Z",
     "start_time": "2025-05-05T01:11:28.645520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "sub_tokens_by_length = defaultdict(list)\n",
    "for word in words:\n",
    "    sub_tokens = tokenizer.tokenize(word)\n",
    "    sub_tokens_by_length[len(sub_tokens)].append((word,sub_tokens))\n",
    "\n",
    "sub_tokens_by_length = dict(sorted(sub_tokens_by_length.items()))"
   ],
   "id": "23077690e51c0ba7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:11:28.664227Z",
     "start_time": "2025-05-05T01:11:28.661356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in sub_tokens_by_length.items():\n",
    "    print(f\"{key} Subword Tokens:\")\n",
    "    for (word, sub_tokens) in value:\n",
    "        print(f\"{word} -> \\t{sub_tokens}\")"
   ],
   "id": "a8f5008c273e7164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Subword Tokens:\n",
      "generalization -> \t['general', 'ization']\n",
      "understand -> \t['under', 'stand']\n",
      "counterintuitive -> \t['counter', 'intuitive']\n",
      "3 Subword Tokens:\n",
      "unhappiness -> \t['un', 'h', 'appiness']\n",
      "caregiver -> \t['care', 'g', 'iver']\n",
      "understandable -> \t['under', 'stand', 'able']\n",
      "misunderstanding -> \t['mis', 'under', 'standing']\n",
      "4 Subword Tokens:\n",
      "uncharacteristic -> \t['unch', 'ar', 'acter', 'istic']\n",
      "disestablishmentarianism -> \t['dis', 'establishment', 'arian', 'ism']\n",
      "5 Subword Tokens:\n",
      "antidisestablishmentarianism -> \t['ant', 'idis', 'establishment', 'arian', 'ism']\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
