{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Basic Chatbot\n",
    "\n",
    "Language generation can be used to create a chatbot. The prompt given to a language model should contain some information about the task, the conversation history and the latest user input.\n",
    "\n",
    "Below is the prompt template with placeholders for the conversation history and input. We'll try this out with `distilgpt2`. However it is a smaller model that will run quickly but not perform very well as a chatbot. In comparison, larger chat-specific language models (including ChatGPT) can adapt well to different inputs and have been trained specifically for conversations."
   ],
   "id": "c1f5b0dfa543a787"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T21:34:20.982608Z",
     "start_time": "2025-05-11T21:34:20.980816Z"
    }
   },
   "source": [
    "chatbot_prompt = \"\"\"\n",
    "You are an advanced chatbot who will try to answer the user's queries honestly.\n",
    "\n",
    "CONVERSATION_HISTORY\n",
    "\n",
    "User: INPUT\n",
    "Chatbot:\"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll load the text generation pipeline using the `distilgpt2` model",
   "id": "2b57cdc390726f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T21:34:21.763492Z",
     "start_time": "2025-05-11T21:34:20.999974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "generator = pipeline('text-generation', model='distilgpt2')"
   ],
   "id": "63fe51e4624dac5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Text generation typically runs until the desired length of text is attained (e.g. with `max_new_tokens`) or an end-of-sequence special token is output. We want our text generation to end when a newline character (e.g. a return) appears, so that the chatbot can only generate a single line of text. For that we'll define a special StoppingCriteria to give to the `generator`.",
   "id": "1ce1879056b70673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T21:34:21.779522Z",
     "start_time": "2025-05-11T21:34:21.776029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "newline_index = tokenizer.encode('\\n')[0]\n",
    "\n",
    "\n",
    "class StopOnNewLine(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Purpose: To stop text generation when a newline character is generated.\n",
    "    Inheritance: Inherits from the StoppingCriteria class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        \"\"\"\n",
    "        The method checks if the last token ID in the sequence matches the token ID for the newline character (newline_index). If it does, the method returns True, signaling the generation process to stop. Otherwise, it returns False, allowing the generation to continue.\n",
    "\n",
    "        :param input_ids: A tensor containing the token IDs generated so far. It represents the sequence of tokens the model has produced.\n",
    "        :param scores: A tensor containing the model's output probabilities for the next token. This is not used in this implementation.\n",
    "        :param kwargs: Additional keyword arguments (not used in this implementation).\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return input_ids[0][-1] == newline_index"
   ],
   "id": "798ae977b3addf8e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T21:34:56.854636Z",
     "start_time": "2025-05-11T21:34:21.790647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation_turns = 0\n",
    "# Start the conversation with a blank slate\n",
    "conversation_history = ''\n",
    "# Run infinitely (press the Stop button to stop this cell)\n",
    "while conversation_turns < 3:\n",
    "    # Solicit input from the user\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Integrate the conversation history and latest user input into the prompt\n",
    "    prompt = chatbot_prompt.replace('CONVERSATION_HISTORY', conversation_history).replace('INPUT', user_input)\n",
    "\n",
    "    print(f\"Conversation turn: {conversation_turns}\")\n",
    "    print(f\"Conversation history: {conversation_history}\")\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"===============Previous context=================\")\n",
    "\n",
    "    # Run the generation pipeline\n",
    "    generated = generator(prompt,\n",
    "                          max_new_tokens=100,\n",
    "                          do_sample=True,  # Use sampling for more interesting output\n",
    "                          return_full_text=False,  # Only return new tokens, not the prompt\n",
    "                          stopping_criteria=StoppingCriteriaList([StopOnNewLine()]),\n",
    "                          # Stop on a new line (or if max_new_tokens is reached)\n",
    "                          pad_token_id=50256,\n",
    "                          )  # Provide that so it doesn't give an annoying warning\n",
    "\n",
    "    # Get the response and output it\n",
    "    response = generated[0]['generated_text'].strip()\n",
    "    print(f\"Chatbot: {response}\\n\")\n",
    "\n",
    "    # Integrate in the latest input/response into the conversation history\n",
    "    conversation_history = f\"\"\"\n",
    "{conversation_history}\n",
    "\n",
    "User: {user_input}\n",
    "Chatbot: {response}\n",
    "\"\"\".strip()\n",
    "    conversation_turns += 1\n"
   ],
   "id": "928a82c784620d55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation turn: 0\n",
      "Conversation history: \n",
      "Prompt: You are an advanced chatbot who will try to answer the user's queries honestly.\n",
      "\n",
      "\n",
      "\n",
      "User: Are you a human?\n",
      "Chatbot:\n",
      "\n",
      "===============Previous context=================\n",
      "Chatbot: Yes.\n",
      "\n",
      "Conversation turn: 1\n",
      "Conversation history: User: Are you a human?\n",
      "Chatbot: Yes.\n",
      "Prompt: You are an advanced chatbot who will try to answer the user's queries honestly.\n",
      "\n",
      "User: Are you a human?\n",
      "Chatbot: Yes.\n",
      "\n",
      "User: Which is stronger an elephant or a dog?\n",
      "Chatbot:\n",
      "\n",
      "===============Previous context=================\n",
      "Chatbot: Yes.\n",
      "\n",
      "Conversation turn: 2\n",
      "Conversation history: User: Are you a human?\n",
      "Chatbot: Yes.\n",
      "\n",
      "User: Which is stronger an elephant or a dog?\n",
      "Chatbot: Yes.\n",
      "Prompt: You are an advanced chatbot who will try to answer the user's queries honestly.\n",
      "\n",
      "User: Are you a human?\n",
      "Chatbot: Yes.\n",
      "\n",
      "User: Which is stronger an elephant or a dog?\n",
      "Chatbot: Yes.\n",
      "\n",
      "User: Which is faster a falcon or a cheetah?\n",
      "Chatbot:\n",
      "\n",
      "===============Previous context=================\n",
      "Chatbot: Yes.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T21:36:38.534085Z",
     "start_time": "2025-05-11T21:36:35.762846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chatbot_prompt = \"\"\"\n",
    "You are an advanced YouTue video content editor. You suggest engaging titles, and help with content brainstorming\n",
    "\n",
    "CONVERSATION_HISTORY\n",
    "\n",
    "User: INPUT\n",
    "Chatbot:\"\"\".strip()\n",
    "\n",
    "user_inputs = [\"Give me titles talking about Rust Programming\",\n",
    "               \"Generate a video script description of one of the titles\",\n",
    "               \"Elaborate more about the content of the video\"]\n",
    "\n",
    "conversation_history = ''\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "for user_input in user_inputs:\n",
    "    prompt = chatbot_prompt.replace('CONVERSATION_HISTORY', conversation_history).replace('INPUT', user_input)\n",
    "    print(\"Prompt before generation:\")\n",
    "    print(prompt)\n",
    "    print(\"==============================\\n\")\n",
    "    generated = generator(prompt,\n",
    "                          max_new_tokens=100,\n",
    "                          do_sample=True,  # Use sampling for more interesting output\n",
    "                          return_full_text=False,  # Only return new tokens, not the prompt\n",
    "                          stopping_criteria=StoppingCriteriaList([StopOnNewLine()]),\n",
    "                          # Stop on a new line (or if max_new_tokens is reached)\n",
    "                          pad_token_id=50256)  # Provide that so it doesn't give an annoying warning\n",
    "\n",
    "    # Get the response and output it\n",
    "    response = generated[0]['generated_text'].strip()\n",
    "    print(f\"Chatbot: {response}\\n\")\n",
    "\n",
    "    # Integrate in the latest input/response into the conversation history\n",
    "    conversation_history = f\"\"\"\n",
    "{conversation_history}\n",
    "\n",
    "User: {user_input}\n",
    "Chatbot: {response}\n",
    "\"\"\".strip()"
   ],
   "id": "72d415f824b348bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt before generation:\n",
      "You are an advanced YouTue video content editor. You suggest engaging titles, and help with content brainstorming\n",
      "\n",
      "\n",
      "\n",
      "User: Give me titles talking about Rust Programming\n",
      "Chatbot:\n",
      "==============================\n",
      "\n",
      "Chatbot: I've been working with Rust development for over a year now which is still kinda nice. So my next project are my projects and I'm having a lot fun with it.\n",
      "\n",
      "Prompt before generation:\n",
      "You are an advanced YouTue video content editor. You suggest engaging titles, and help with content brainstorming\n",
      "\n",
      "User: Give me titles talking about Rust Programming\n",
      "Chatbot: I've been working with Rust development for over a year now which is still kinda nice. So my next project are my projects and I'm having a lot fun with it.\n",
      "\n",
      "User: Generate a video script description of one of the titles\n",
      "Chatbot:\n",
      "==============================\n",
      "\n",
      "Chatbot: I'm a freelance developer who is getting into Visual Studio, so there's definitely no easy way to write a video. You can also use the text or use the video editor.\n",
      "\n",
      "Prompt before generation:\n",
      "You are an advanced YouTue video content editor. You suggest engaging titles, and help with content brainstorming\n",
      "\n",
      "User: Give me titles talking about Rust Programming\n",
      "Chatbot: I've been working with Rust development for over a year now which is still kinda nice. So my next project are my projects and I'm having a lot fun with it.\n",
      "\n",
      "User: Generate a video script description of one of the titles\n",
      "Chatbot: I'm a freelance developer who is getting into Visual Studio, so there's definitely no easy way to write a video. You can also use the text or use the video editor.\n",
      "\n",
      "User: Elaborate more about the content of the video\n",
      "Chatbot:\n",
      "==============================\n",
      "\n",
      "Chatbot: You don't want to build your own custom video script on the server for your computer. I want a simpler way of getting started as an artist.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations\n",
    "\n",
    "There are numerous limitations with this basic approach\n",
    "\n",
    "- The `distilgpt2` model is small and not optimised for chat so it won't perform that well in this context\n",
    "- The model has a smaller context window and there isn't any checking to see if the prompt is too large for the model. This will happen when the `conversation_history` gets too long. Then the text generation won't function as expected and meaningless responses may be made.\n",
    "- A lot of tweaks could be made to the parameters used for text generation (e.g. temperature, top_k, etc) to get better results"
   ],
   "id": "73ce60e4af1e196e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
