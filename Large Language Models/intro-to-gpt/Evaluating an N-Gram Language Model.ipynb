{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluating an N-Gram Language Model\n",
    "\n"
   ],
   "id": "7375c02caf7cc6dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:46:52.964750Z",
     "start_time": "2025-05-01T17:46:52.587107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from n_gram import NGramLM\n",
    "\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "\n",
    "# Load pre-built n-gram languae models\n",
    "model_unigram = NGramLM('arthur-conan-doyle.tok.train.n1.pkl', .01, verbose=True)\n",
    "model_bigram = NGramLM('arthur-conan-doyle.tok.train.n2.pkl', .01)\n",
    "model_trigram = NGramLM('arthur-conan-doyle.tok.train.n3.pkl', .01)\n",
    "model_4gram = NGramLM('arthur-conan-doyle.tok.train.n4.pkl', .01)\n",
    "model_5gram = NGramLM('arthur-conan-doyle.tok.train.n5.pkl', .01)"
   ],
   "id": "24f979c9398e1052",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now it's time to see how well these models fit our data! We'll use Perplexity for this calculation, but it's up to you to implement it below.\n",
    "\n",
    "Recall the formula for perplexity from the lecture:\n",
    "\n",
    "$$\n",
    "perplexity = 2^{\\frac{-1}{n}\\sum \\log_2(P(w_i|w_{<i}))}\n",
    "$$\n",
    "\n",
    "Hint: you'll want to use the [`math.log2`](https://docs.python.org/3/library/math.html#math.log2) function"
   ],
   "id": "cd11f165e1834713"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:46:53.092156Z",
     "start_time": "2025-05-01T17:46:53.089516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def perplexity(model: NGramLM, texts: list[tuple[str]]) -> float:\n",
    "    n_word = sum(len(text) for text in texts)\n",
    "    res = sum(\n",
    "        math.log2(model.get_prob(text[:i], text[i]))\n",
    "        for text in texts\n",
    "        for i in range(len(text))\n",
    "    )\n",
    "\n",
    "    return math.pow(2, -1 / n_word * res)\n",
    "\n",
    "\n",
    "model_unigram.verbose = True\n",
    "print(perplexity(model_unigram, [('My', 'dear', 'Watson', '.'), ('Come', 'over', 'here', '!')]))\n",
    "model_unigram.verbose = False"
   ],
   "id": "adc3d0d24c6904c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006603 -> My\n",
      "5.955e-05 My -> dear\n",
      "5.955e-05 My dear -> Watson\n",
      "5.955e-05 My dear Watson -> .\n",
      "0.0001687 -> Come\n",
      "5.955e-05 Come -> over\n",
      "5.955e-05 Come over -> here\n",
      "5.955e-05 Come over here -> !\n",
      "10914.060522177839\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:46:53.098816Z",
     "start_time": "2025-05-01T17:46:53.096936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tests\n",
    "assert round(perplexity(model_unigram, [('My', 'dear', 'Watson')])) == 7531\n",
    "assert round(perplexity(model_bigram, [('My', 'dear', 'Watson')])) == 24\n",
    "assert round(perplexity(model_trigram, [('My', 'dear', 'Watson')])) == 521"
   ],
   "id": "64fcf5de6945c39c",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
