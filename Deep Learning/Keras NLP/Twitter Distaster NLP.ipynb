{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=45, stratify=train_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"keyword\"].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"location\"].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data:pd.DataFrame):\n",
    "    data.drop(columns=[\"location\", \"id\"], inplace=True)\n",
    "    data.fillna(\"\",inplace=True)    \n",
    "\n",
    "clean_data(train_data)\n",
    "clean_data(test_data)\n",
    "clean_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data.pop(\"target\")\n",
    "valid_labels = valid_data.pop(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No disaster: {train_data[train_labels == 0][\"text\"].values[1]}')\n",
    "print(f'Disaster: {train_data[train_labels == 1][\"text\"].values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\",train_data.shape)\n",
    "print(\"Test shape:\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = train_labels.value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(target_counts.index, target_counts.values)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(target_counts.index, ['Not Disaster', 'Disaster'])\n",
    "plt.title('Distribution of Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Regular expression pattern to match URLs\n",
    "pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "def substitute_links(text):\n",
    "    \n",
    "    # Substitute links in the text with \"website: domain_name\"\n",
    "    substituted_text = re.sub(pattern, 'website: domain_name', text)\n",
    "\n",
    "    return substituted_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Check out this website: https://example.com and also visit http://openai.com\"\n",
    "\n",
    "substituted_text = substitute_links(text)\n",
    "print(substituted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 0\n",
    "max_sequence_length_keyword = 0\n",
    "# Iterate over text and keyword columns\n",
    "for text, keyword in zip(train_data['text'], train_data['keyword']):\n",
    "    text_length = len(text.split())\n",
    "    keyword_length = len(keyword.split())\n",
    "\n",
    "    # Update maximum sequence length\n",
    "    max_sequence_length = max(max_sequence_length, text_length, keyword_length)\n",
    "    \n",
    "for keyword in train_data['keyword']:\n",
    "    keyword_length = len(keyword.split())\n",
    "\n",
    "    # Update maximum sequence length\n",
    "    max_sequence_length_keyword = max(max_sequence_length_keyword, keyword_length)\n",
    "\n",
    "print(\"Maximum Sequence Length:\", max_sequence_length)\n",
    "print(\"Maximum Sequence Length Keyword:\", max_sequence_length_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Tokenize text and keyword columns\n",
    "text_tokenizer = Tokenizer()\n",
    "text_tokenizer.fit_on_texts(train_data['text'])\n",
    "train_text_sequences = text_tokenizer.texts_to_sequences(train_data['text'])\n",
    "\n",
    "keyword_tokenizer = Tokenizer()\n",
    "keyword_tokenizer.fit_on_texts(train_data['keyword'])\n",
    "train_keyword_sequences = keyword_tokenizer.texts_to_sequences(train_data['keyword'])\n",
    "\n",
    "def process_data(data:pd.DataFrame):\n",
    "    #Tokenize\n",
    "    text_sequences = text_tokenizer.texts_to_sequences(data[\"text\"])\n",
    "    keyword_sequences = keyword_tokenizer.texts_to_sequences(data[\"text\"])\n",
    "    \n",
    "    #Pad\n",
    "    text_sequences = pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "    keyword_sequences = pad_sequences(keyword_sequences, maxlen=max_sequence_length_keyword)\n",
    "    return text_sequences, keyword_sequences\n",
    "\n",
    "# Step 2: Pad sequences\n",
    "# Adjust this value based on your data\n",
    "train_text_sequences, train_keyword_sequences = process_data(train_data)\n",
    "\n",
    "# Step 3: Create the dataset\n",
    "labels = train_labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((train_text_sequences, train_keyword_sequences), labels))\n",
    "dataset = dataset.shuffle(1000).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_index = 1\n",
    "print(train_data[\"text\"][0].split()[w_index])\n",
    "print(text_tokenizer.texts_to_sequences(train_data[\"text\"][0].split()[w_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[\"text\"][0])\n",
    "print(train_data[\"text\"][0].split())\n",
    "print(len(train_data[\"text\"][0].split()))\n",
    "print(len(text_tokenizer.texts_to_sequences(train_data[\"text\"][0])))\n",
    "text_tokenizer.texts_to_sequences(train_data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs\n",
    "input_text = Input(shape=(max_sequence_length,), name='input_text')\n",
    "input_keyword = Input(shape=(max_sequence_length_keyword,), name='input_keyword')\n",
    "\n",
    "# Text part\n",
    "embedding = Embedding(name=\"embedding\",input_dim=len(text_tokenizer.word_index) + 1, output_dim=16, input_length=max_sequence_length)(input_text)\n",
    "lstm_text = Bidirectional(LSTM(16, return_sequences=True))(embedding)\n",
    "\n",
    "# Keyword part\n",
    "dense_keyword = Dense(8, activation='relu')(input_keyword)\n",
    "dense_keyword_repeated = RepeatVector(max_sequence_length)(dense_keyword)\n",
    "\n",
    "# Concatenate the outputs\n",
    "concatenated = Concatenate(axis=-1)([lstm_text, dense_keyword_repeated])\n",
    "\n",
    "# Other layers\n",
    "flatten = Flatten()(concatenated)\n",
    "dense = Dense(16, activation='relu')(flatten)\n",
    "dropout = Dropout(0.15)(dense)\n",
    "dense2 = Dense(8, activation='relu')(dropout)\n",
    "dropout2 = Dropout(0.25)(dense2)\n",
    "output = Dense(2, activation='softmax')(dropout2)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=[input_text, input_keyword], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using multiple output\n",
    "labels = to_categorical(labels, num_classes=2)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_text, valid_keyword = process_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "opt = tf.keras.optimizers.Adam(0.001) #75\n",
    "# opt = tf.keras.optimizers.Adagrad(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "h = model.fit(\n",
    "    {'input_text': train_text_sequences, 'input_keyword': train_keyword_sequences}, \n",
    "    labels, \n",
    "    validation_data=(\n",
    "        {'input_text': valid_text, 'input_keyword': valid_keyword},\n",
    "        valid_labels\n",
    "        ),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = h.history\n",
    "print(history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_range = range(1, len(history['loss'])+1)\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch_range, history['loss'], label='Training')\n",
    "plt.plot(epoch_range, history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch_range, history['accuracy'], label='Training')\n",
    "plt.plot(epoch_range, history['val_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_tokenizer.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
