{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks\n",
    "This notebook has mixed types of theoretical and code implementation questions on multilayer perceptron and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "**Part A** : Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights and biases as applicable:\n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$\n",
    "\n",
    "ii. $~~x_1 \\texttt{ NOR } x_2$\n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B** : Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. <br>\n",
    "In this week's Peer Review, describe your architecture and state your weight matrices and bias vectors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$. <br>\n",
    "By reading [Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering the following thruth tabel <br>\n",
    "![XNOR truth tabel](https://miro.medium.com/v2/resize:fit:598/0*oGu2x1DA9soE3IdO.gif) <br>\n",
    "And the following neural network <br>\n",
    "![XNOR neural network](https://miro.medium.com/v2/resize:fit:640/format:webp/1*yZfw_9DRMephzZwejjhyTA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| a | b | nor(and,nor) => xor\n",
      "| 0 | 0 | nor(  0,  1) =>   0\n",
      "| 0 | 1 | nor(  0,  0) =>   1\n",
      "| 1 | 0 | nor(  0,  0) =>   1\n",
      "| 1 | 1 | nor(  1,  0) =>   0\n"
     ]
    }
   ],
   "source": [
    "# implement forward propagation for network\n",
    "# show that it correctly produces the correct boolean output values \n",
    "# for each of the four possible combinations of x1 and x2 \n",
    "\n",
    "def neuron(X:np.array, W:np.array, b:float):\n",
    "    return activation(X@W + b)\n",
    "# Initialize x with the 4 possible combinations of 0 and 1 to generate 4 values for y(output)\n",
    "def activation(x):\n",
    "    return 1 if x>0 else 0\n",
    "def and_neuron(X:np.array):\n",
    "    b = -1\n",
    "    W = np.array([1,1])\n",
    "    return neuron(X,W,b)\n",
    "def not_neuron(X):\n",
    "    W = np.array([-1])\n",
    "    X = np.array([X])\n",
    "    return neuron(X,W,1)\n",
    "def nor_neuron(X:np.array):\n",
    "    W = np.array([-1,-1])\n",
    "    b = 1\n",
    "    return neuron(X,W,b)\n",
    "def xor_neuron(X:np.array):\n",
    "    x1 = and_neuron(X)\n",
    "    x2 = nor_neuron(X)\n",
    "    return nor_neuron(np.array([x1,x2]))\n",
    "def convert_to_array(i):\n",
    "    raw = [int(j) for j in f\"{i:02b}\"]\n",
    "    return np.array(raw)\n",
    "\n",
    "\n",
    "combinations = [ convert_to_array(i) for i in range(4)]\n",
    "table_format = \"| {:1} | {:1} | nor({:3},{:3}) => {:3}\"\n",
    "header = table_format.format(\"a\",\"b\", \"and\", \"nor\", \"xor\")\n",
    "print(header)\n",
    "for i in combinations:\n",
    "    print(table_format.format(i[0], i[1], and_neuron(i), nor_neuron(i), xor_neuron(i)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 - Back propagation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. Answer the Peer Review question about this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. Answer the Peer Review question about this section. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART C** Implement following activation functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Relu: f($x$) = max(0, $x$)\n",
    "<br><br>\n",
    "\n",
    "* Sigmoid: f($x$) = $\\frac{1}{1 + e^{-x}}$\n",
    "<br><br>\n",
    "\n",
    "* Softmax: f($x_i$) = $\\frac{e^x_i}{\\sum_{j=1}^{n} e^{x_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def soft_max(x):\n",
    "    x_exp = [math.exp(xi) for xi in x]\n",
    "    den = sum(x_exp)\n",
    "    return [xei/den for xei in x_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "# Test Relu function\n",
    "assert int(relu(-6.5)) == 0, \"Check relu function\"\n",
    "\n",
    "# Test Sigmoid function\n",
    "assert pytest.approx(sigmoid(0.3), 0.00001) == 0.574442516811659, \"Check sigmoid function\"\n",
    "\n",
    "# Test Softmax function\n",
    "assert pytest.approx(soft_max([5,7]), 0.00001) == [0.11920292, 0.88079708], \"Check softmax function\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART D** Implement the following Loss functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Mean squared error <br>\n",
    "Formula: MSE = (1/n) * Σ(yi - ŷi)^2\n",
    "\n",
    "* Mean absolute error <br>\n",
    "Formula: MAE = (1/n) * Σ|yi - ŷi|\n",
    "\n",
    "* Hinge Loss <br>\n",
    "Formula: L = max(0, 1 - yi * ŷi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat,y):\n",
    "    n = len(y)\n",
    "    dif_square = [(yi-yhi)**2 for yi, yhi in zip(y,yhat)]\n",
    "    return (1/n)*sum(dif_square)\n",
    "    \n",
    "def mean_absolute_error(yhat,y):\n",
    "    n = len(y)\n",
    "    dif_abs = [abs(yi-yhi) for yi, yhi in zip(y,yhat)]\n",
    "    return (1/n)*sum(dif_abs)\n",
    "    \n",
    "\n",
    "def hinge(yhat,y):\n",
    "    li = [max(0,1-yi*yhi) for yi, yhi in zip(y,yhat) ]\n",
    "    return sum(li)/len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "y_true = np.array([2, 3, -0.45])\n",
    "y_pred = np.array([1.5, 3, 0.2])\n",
    "\n",
    "# Test mean squared error function\n",
    "assert pytest.approx(mean_squared_error(y_pred,y_true), 0.00001) == 0.2241666666666667, \"Check mean_squared_error function\"\n",
    "\n",
    "# Test mean absolute error function\n",
    "assert pytest.approx(mean_absolute_error(y_pred,y_true), 0.00001) == 0.3833333333333333, \"Check mean_absolute_error function\"\n",
    "\n",
    "# Test hinge loss function\n",
    "assert pytest.approx(hinge(y_pred,y_true), 0.00001) == 0.36333333333333334, \"Check hinge loss function\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
