{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks\n",
    "This notebook has mixed types of theoretical and code implementation questions on multilayer perceptron and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpylab\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import datasets\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "**Part A** : Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights and biases as applicable:\n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$\n",
    "\n",
    "ii. $~~x_1 \\texttt{ NOR } x_2$\n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B** : Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. <br>\n",
    "In this week's Peer Review, describe your architecture and state your weight matrices and bias vectors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$. <br>\n",
    "By reading [Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering the following thruth tabel <br>\n",
    "![XNOR truth tabel](https://miro.medium.com/v2/resize:fit:598/0*oGu2x1DA9soE3IdO.gif) <br>\n",
    "And the following neural network <br>\n",
    "![XNOR neural network](https://miro.medium.com/v2/resize:fit:640/format:webp/1*yZfw_9DRMephzZwejjhyTA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| a | b | nor(and,nor) => xor\n",
      "| 0 | 0 | nor(  0,  1) =>   0\n",
      "| 0 | 1 | nor(  0,  0) =>   1\n",
      "| 1 | 0 | nor(  0,  0) =>   1\n",
      "| 1 | 1 | nor(  1,  0) =>   0\n"
     ]
    }
   ],
   "source": [
    "# implement forward propagation for network\n",
    "# show that it correctly produces the correct boolean output values \n",
    "# for each of the four possible combinations of x1 and x2 \n",
    "\n",
    "def neuron(X:np.array, W:np.array, b:float):\n",
    "    return activation(X@W + b)\n",
    "# Initialize x with the 4 possible combinations of 0 and 1 to generate 4 values for y(output)\n",
    "def activation(x):\n",
    "    return 1 if x>0 else 0\n",
    "def and_neuron(X:np.array):\n",
    "    b = -1\n",
    "    W = np.array([1,1])\n",
    "    return neuron(X,W,b)\n",
    "def not_neuron(X):\n",
    "    W = np.array([-1])\n",
    "    X = np.array([X])\n",
    "    return neuron(X,W,1)\n",
    "def nor_neuron(X:np.array):\n",
    "    W = np.array([-1,-1])\n",
    "    b = 1\n",
    "    return neuron(X,W,b)\n",
    "def xor_neuron(X:np.array):\n",
    "    x1 = and_neuron(X)\n",
    "    x2 = nor_neuron(X)\n",
    "    return nor_neuron(np.array([x1,x2]))\n",
    "def convert_to_array(i):\n",
    "    raw = [int(j) for j in f\"{i:02b}\"]\n",
    "    return np.array(raw)\n",
    "\n",
    "\n",
    "combinations = [ convert_to_array(i) for i in range(4)]\n",
    "table_format = \"| {:1} | {:1} | nor({:3},{:3}) => {:3}\"\n",
    "header = table_format.format(\"a\",\"b\", \"and\", \"nor\", \"xor\")\n",
    "print(header)\n",
    "for i in combinations:\n",
    "    print(table_format.format(i[0], i[1], and_neuron(i), nor_neuron(i), xor_neuron(i)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 - Back propagation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. Answer the Peer Review question about this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. Answer the Peer Review question about this section. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART C** Implement following activation functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Relu: f($x$) = max(0, $x$)\n",
    "<br><br>\n",
    "\n",
    "* Sigmoid: f($x$) = $\\frac{1}{1 + e^{-x}}$\n",
    "<br><br>\n",
    "\n",
    "* Softmax: f($x_i$) = $\\frac{e^x_i}{\\sum_{j=1}^{n} e^{x_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def soft_max(x):\n",
    "    x_exp = [math.exp(xi) for xi in x]\n",
    "    den = sum(x_exp)\n",
    "    return [xei/den for xei in x_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "# Test Relu function\n",
    "assert int(relu(-6.5)) == 0, \"Check relu function\"\n",
    "\n",
    "# Test Sigmoid function\n",
    "assert pytest.approx(sigmoid(0.3), 0.00001) == 0.574442516811659, \"Check sigmoid function\"\n",
    "\n",
    "# Test Softmax function\n",
    "assert pytest.approx(soft_max([5,7]), 0.00001) == [0.11920292, 0.88079708], \"Check softmax function\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART D** Implement the following Loss functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Mean squared error <br>\n",
    "Formula: MSE = (1/n) * Σ(yi - ŷi)^2\n",
    "\n",
    "* Mean absolute error <br>\n",
    "Formula: MAE = (1/n) * Σ|yi - ŷi|\n",
    "\n",
    "* Hinge Loss <br>\n",
    "Formula: L = max(0, 1 - yi * ŷi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat,y):\n",
    "    n = len(y)\n",
    "    dif_square = [(yi-yhi)**2 for yi, yhi in zip(y,yhat)]\n",
    "    return (1/n)*sum(dif_square)\n",
    "    \n",
    "def mean_absolute_error(yhat,y):\n",
    "    n = len(y)\n",
    "    dif_abs = [abs(yi-yhi) for yi, yhi in zip(y,yhat)]\n",
    "    return (1/n)*sum(dif_abs)\n",
    "    \n",
    "\n",
    "def hinge(yhat,y):\n",
    "    li = [max(0,1-yi*yhi) for yi, yhi in zip(y,yhat) ]\n",
    "    return sum(li)/len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "y_true = np.array([2, 3, -0.45])\n",
    "y_pred = np.array([1.5, 3, 0.2])\n",
    "\n",
    "# Test mean squared error function\n",
    "assert pytest.approx(mean_squared_error(y_pred,y_true), 0.00001) == 0.2241666666666667, \"Check mean_squared_error function\"\n",
    "\n",
    "# Test mean absolute error function\n",
    "assert pytest.approx(mean_absolute_error(y_pred,y_true), 0.00001) == 0.3833333333333333, \"Check mean_absolute_error function\"\n",
    "\n",
    "# Test hinge loss function\n",
    "assert pytest.approx(hinge(y_pred,y_true), 0.00001) == 0.36333333333333334, \"Check hinge loss function\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 - Build a feed-forward neural network\n",
    "---\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement forward propagation, prediction, back propagation, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "\n",
    "The skeleton for the network class is below. Befor filling out the codes below, read the PART X instruction. The place you will complete the code is indicated as \"TODO\" in the code. Pleaes do not modify other parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros((n, 1)) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def grad_loss(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        # TODO: step 1. Initialize activation on initial layer to x \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        ## TODO: step 2-4. Loop over layers and compute activities and activations \n",
    "        ## Use Sigmoid activation function defined above\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # TODO: step 1. forward prop training example to fill in activities and activations \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # TODO: step 2. compute deltas on output layer (Hint: python index numbering starts from 0 ends at N-1)\n",
    "        # Correction in Instructions: From the instructions mentioned below for backward propagation,\n",
    "        # Use normal product instead of dot product in Step 2 and 6\n",
    "        # The derivative and gradient functions have already been implemented for you\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # TODO: step 3-6. loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        # your code here\n",
    "        \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None,\n",
    "              eta=0.25, num_epochs=10, isPrint=True, isVis=False):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs (step 1.)\n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples (step 2.) \n",
    "            for ind in shuffled_inds: \n",
    "                \n",
    "                # TODO: step 3. back prop to get derivatives \n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "                # TODO: step 4. update all weights and biases for all layers\n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "            # print mean loss every 10 epochs if requested \n",
    "            if isPrint and (ep % 10) == 0:\n",
    "                print(\"epoch {:3d}/{:3d}: \".format(ep, num_epochs), end=\"\")\n",
    "                print(\"  train loss: {:8.3f}\".format(self.compute_loss(X_train, y_train)), end=\"\")\n",
    "                if X_valid is not None:\n",
    "                    print(\"  validation loss: {:8.3f}\".format(self.compute_loss(X_valid, y_valid)))\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    \n",
    "            if isVis and (ep % 20) == 0:\n",
    "                self.pretty_pictures(X_train, y_train, decision_boundary=True, epoch=ep)\n",
    "                    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        compute average loss for given data set \n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        if len(y.shape) == 1:\n",
    "            y = y[np.newaxis, :]\n",
    "        for x, t in zip(X, y):\n",
    "            self.forward_prop(x)\n",
    "            if len(t.shape) == 1:\n",
    "                t = t.reshape(-1, 1)\n",
    "            loss += 0.5 * np.sum((self.a[-1] - t) ** 2)\n",
    "        return loss / X.shape[0]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h=1e-5):\n",
    "        \"\"\"\n",
    "        check whether the gradient is correct for X, y\n",
    "        \n",
    "        Assuming that back_prop has finished.\n",
    "        \"\"\"\n",
    "        for ll in range(self.L - 1):\n",
    "            oldW = self.W[ll].copy()\n",
    "            oldb = self.b[ll].copy()\n",
    "            for i in range(self.W[ll].shape[0]):\n",
    "                for j in range(self.W[ll].shape[1]):\n",
    "                    self.W[ll][i, j] = oldW[i, j] + h\n",
    "                    lxph = self.compute_loss(x, y)\n",
    "                    self.W[ll][i, j] = oldW[i, j] - h\n",
    "                    lxmh = self.compute_loss(x, y)\n",
    "                    grad = (lxph - lxmh) / (2 * h)\n",
    "                    assert abs(self.dW[ll][i, j] - grad) < 1e-5\n",
    "                    self.W[ll][i, j] = oldW[i, j]\n",
    "            for i in range(self.b[ll].shape[0]):\n",
    "                j = 0\n",
    "                self.b[ll][i, j] = oldb[i, j] + h\n",
    "                lxph = self.compute_loss(x, y)\n",
    "                self.b[ll][i, j] = oldb[i, j] - h\n",
    "                lxmh = self.compute_loss(x, y)\n",
    "                grad = (lxph - lxmh) / (2 * h)\n",
    "                assert abs(self.db[ll][i, j] - grad) < 1e-5\n",
    "                self.b[ll][i, j] = oldb[i, j]\n",
    "        \n",
    "            \n",
    "    def pretty_pictures(self, X, y, decision_boundary=False, epoch=None):\n",
    "        \"\"\"\n",
    "        Function to plot data and neural net decision boundary\n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        :param decision_boundary: whether or not to plot decision \n",
    "        :param epoch: epoch number for printing \n",
    "        \"\"\"\n",
    "        \n",
    "        mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\"}\n",
    "        colorlist = [c for (n,c) in mycolors.items()]\n",
    "        colors = [colorlist[np.argmax(yk)] for yk in y]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "        \n",
    "        if decision_boundary:\n",
    "            xx, yy = np.meshgrid(np.linspace(-1.25,1.25,300), np.linspace(-1.25,1.25,300))\n",
    "            grid = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "            grid_pred = np.zeros_like(grid[:,0])\n",
    "            for ii in range(len(grid_pred)):\n",
    "                self.forward_prop(grid[ii,:])\n",
    "                grid_pred[ii] = np.argmax(self.a[-1])\n",
    "            grid_pred = grid_pred.reshape(xx.shape)\n",
    "            cmap = ListedColormap([\n",
    "                colorConverter.to_rgba('steelblue', alpha=0.30),\n",
    "                colorConverter.to_rgba('#a76c63', alpha=0.30)])\n",
    "            plt.contourf(xx, yy, grid_pred, cmap=cmap)\n",
    "            if epoch is not None: plt.text(-1.23,1.15, \"epoch = {:d}\".format(epoch), fontsize=16)\n",
    "\n",
    "        plt.scatter(X[:,0], X[:,1], color=colors, s=100, alpha=0.9)\n",
    "        plt.axis('off')\n",
    "        \n",
    "def generate_data(N, config=\"checkerboard\"):\n",
    "    X = np.zeros((N,2))\n",
    "    y = np.zeros((N,2)).astype(int)\n",
    "    \n",
    "    if config==\"checkerboard\":\n",
    "        nps, sqlen = N//9, 2/3\n",
    "        ctr = 0\n",
    "        for ii in range(3):\n",
    "            for jj in range(3):\n",
    "                X[ctr * nps : (ctr + 1) * nps, :] = np.column_stack(\n",
    "                    (np.random.uniform(ii * sqlen +.05-1, (ii+1) * sqlen - .05 -1, size=nps),\n",
    "                     np.random.uniform(jj * sqlen +.05-1, (jj+1) * sqlen - .05 -1, size=nps))) \n",
    "                y[ctr*nps:(ctr+1)*nps,(3*ii+jj)%2] = 1 \n",
    "                ctr += 1\n",
    "                \n",
    "    if config==\"blobs\":            \n",
    "        X, yflat = datasets.make_blobs(n_samples=N, centers=[[-.5,.5],[.5,-.5]],\n",
    "                                       cluster_std=[.20,.20],n_features=2)\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk,:] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    \n",
    "    if config==\"circles\":\n",
    "        kk=0\n",
    "        while kk < N / 2:\n",
    "            sample = 2 * np.random.rand(2) - 1 \n",
    "            if np.linalg.norm(sample) <= .45:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([1,0])\n",
    "                kk += 1 \n",
    "        while kk < N:\n",
    "            sample = 2 * np.random.rand(2) - 1\n",
    "            dist = np.linalg.norm(sample)\n",
    "            if dist < 0.9 and dist > 0.55:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([0,1])\n",
    "                kk += 1\n",
    "                \n",
    "    if config==\"moons\":\n",
    "        X, yflat = datasets.make_moons(n_samples=N, noise=.05)\n",
    "        X[:,0] = .5 * (X[:,0] - .5)\n",
    "        X[:,1] = X[:,1] - .25\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk, :] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using our network to do binary classification of two-dimensional feature vectors.  Scroll down to the **Helper Functions** and examine the function ``generate_data``. Then mess around with the following cell to look at the various data sets available.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
