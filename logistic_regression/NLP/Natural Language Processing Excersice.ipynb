{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading AG News with Torchtext\n",
    "\n",
    "The AG News dataset is one of many included Torchtext.\n",
    "It can be found grouped together with many of the other text classification datasets.\n",
    "While we can download the source text online, Torchtext makes it retrievable with a quick API call&ast;. If you are running this notebook on your machine,  you can uncomment and run this block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.datasets.ag_news import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = AG_NEWS(split=(\"train\", \"test\"))\n",
    "flag, text = next(iter(train_iter)) #It each iterator has the category/value and text\n",
    "tokenizer = get_tokenizer('basic_english') # It retrieves the tool which allows you to encode the text\n",
    "print(text)\n",
    "print(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter): #Converts every text into an array of tokens, it returns it in an iterable way\n",
    "    for _, text in data_iter: \n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"]) #It generates the vocab, which represents the words into indexes\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is yield in Python? [link](https://www.simplilearn.com/tutorials/python-tutorial/yield-in-python#:~:text=let's%20get%20started.-,What%20Is%20Yield%20In%20Python%3F,of%20simply%20returning%20a%20value)<br><br>The Yield keyword in Python is similar to a return statement used for returning values or objects in Python. However, there is a slight difference. The **yield statement returns a generator object to the one who calls the function which contains yield**, instead of simply returning a value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475, 21, 30, 5297]\n"
     ]
    }
   ],
   "source": [
    "print(vocab(['here', 'is', 'an', 'example']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475, 21, 2, 30, 5297]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(text_pipeline('here is the an example'))\n",
    "print(label_pipeline('10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super(SWEM,self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_size, sparse=True)\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x,offsets):\n",
    "        embed = self.embedding(x,offsets)\n",
    "        h = self.fc1(embed)\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEM(\n",
      "  (embedding): EmbeddingBag(95811, 100, mode=mean)\n",
      "  (fc1): Linear(in_features=100, out_features=60, bias=True)\n",
      "  (fc2): Linear(in_features=60, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 60\n",
    "NUM_OUTPUTS = len(set([label for (label, text) in train_iter]))\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "model = SWEM(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    embedding_size = EMBED_DIM, \n",
    "    hidden_dim = HIDDEN_DIM, \n",
    "    num_outputs = NUM_OUTPUTS,\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 2280 batches | accuracy    0.716\n",
      "| epoch   1 |  1000/ 2280 batches | accuracy    0.849\n",
      "| epoch   1 |  1500/ 2280 batches | accuracy    0.874\n",
      "| epoch   1 |  2000/ 2280 batches | accuracy    0.878\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.59s | valid accuracy    0.878 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 2280 batches | accuracy    0.901\n",
      "| epoch   2 |  1000/ 2280 batches | accuracy    0.901\n",
      "| epoch   2 |  1500/ 2280 batches | accuracy    0.905\n",
      "| epoch   2 |  2000/ 2280 batches | accuracy    0.904\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.59s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 2280 batches | accuracy    0.920\n",
      "| epoch   3 |  1000/ 2280 batches | accuracy    0.917\n",
      "| epoch   3 |  1500/ 2280 batches | accuracy    0.917\n",
      "| epoch   3 |  2000/ 2280 batches | accuracy    0.918\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.57s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 2280 batches | accuracy    0.929\n",
      "| epoch   4 |  1000/ 2280 batches | accuracy    0.928\n",
      "| epoch   4 |  1500/ 2280 batches | accuracy    0.927\n",
      "| epoch   4 |  2000/ 2280 batches | accuracy    0.925\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.25s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 2280 batches | accuracy    0.951\n",
      "| epoch   5 |  1000/ 2280 batches | accuracy    0.954\n",
      "| epoch   5 |  1500/ 2280 batches | accuracy    0.955\n",
      "| epoch   5 |  2000/ 2280 batches | accuracy    0.957\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.45s | valid accuracy    0.920 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 2280 batches | accuracy    0.958\n",
      "| epoch   6 |  1000/ 2280 batches | accuracy    0.958\n",
      "| epoch   6 |  1500/ 2280 batches | accuracy    0.960\n",
      "| epoch   6 |  2000/ 2280 batches | accuracy    0.956\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 13.38s | valid accuracy    0.923 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 2280 batches | accuracy    0.960\n",
      "| epoch   7 |  1000/ 2280 batches | accuracy    0.961\n",
      "| epoch   7 |  1500/ 2280 batches | accuracy    0.958\n",
      "| epoch   7 |  2000/ 2280 batches | accuracy    0.961\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.00s | valid accuracy    0.923 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 2280 batches | accuracy    0.964\n",
      "| epoch   8 |  1000/ 2280 batches | accuracy    0.963\n",
      "| epoch   8 |  1500/ 2280 batches | accuracy    0.962\n",
      "| epoch   8 |  2000/ 2280 batches | accuracy    0.965\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.23s | valid accuracy    0.922 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 2280 batches | accuracy    0.966\n",
      "| epoch   9 |  1000/ 2280 batches | accuracy    0.963\n",
      "| epoch   9 |  1500/ 2280 batches | accuracy    0.965\n",
      "| epoch   9 |  2000/ 2280 batches | accuracy    0.964\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.33s | valid accuracy    0.923 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 2280 batches | accuracy    0.965\n",
      "| epoch  10 |  1000/ 2280 batches | accuracy    0.962\n",
      "| epoch  10 |  1500/ 2280 batches | accuracy    0.965\n",
      "| epoch  10 |  2000/ 2280 batches | accuracy    0.966\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.13s | valid accuracy    0.923 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 50 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.918\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "769eaced19ad32e1b9a87390451223e7fa09e6e7964db8015ea6ac7f05d15d9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
