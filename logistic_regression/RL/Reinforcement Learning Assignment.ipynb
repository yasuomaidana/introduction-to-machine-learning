{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Assignment: Reinforcement Learning (RL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. One of the fundamental challenges of reinforcement learning is balancing *exploration* versus *exploitation*. What do these two terms mean, and why do they present a challenge?<br>I will define exploration as trying new posibilities/options instead current optimum known way. On the other hand exploitation is to take advantage of your current knowledge to choose the most optimum way. Now, the trade of comes from the dilema of choosing a current known way or risking into trying something new.\n",
    "\n",
    "\n",
    "2\\. Another fundamental reinforcement learning challenge is what is known as the *credit assignment problem*, especially when rewards are sparse. What do we mean by the phrase, and why does it make learning especially difficult?\n",
    "\n",
    "In reinforcement learning (RL), an agent interacts with an environment in time steps. On each time step, the agent takes an action in a certain state and the environment emits a percept or perception, which is composed of a reward and an observation, which, in the case of fully-observable MDPs, is the next state (of the environment and the agent). The goal of the agent is to maximise the reward in the long run.\n",
    "\n",
    "The (temporal) credit assignment problem (CAP) (discussed in Steps Toward Artificial Intelligence by Marvin Minsky in 1961) is the problem of determining the actions that lead to a certain outcome.\n",
    "\n",
    "For example, in football, at each second, each football player takes an action. In this context, an action can e.g. be \"pass the ball\", \"dribbe\", \"run\" or \"shoot the ball\". At the end of the football match, the outcome can either be a victory, a loss or a tie. After the match, the coach talks to the players and analyses the match and the performance of each player. He discusses the contribution of each player to the result of the match. The problem of determinig the contribution of each player to the result of the match is the (temporal) credit assignment problem.\n",
    "\n",
    "How is this related to RL? In order to maximise the reward in the long run, the agent needs to determine which actions will lead to such outcome, which is essentially the temporal CAP.\n",
    "\n",
    "Why is it called credit assignment problem? In this context, the word credit is a synonym for value. In RL, an action that leads to a higher final cumulative reward should have more value (so more \"credit\" should be assigned to it) than an action that leads to a lower final reward.\n",
    "\n",
    "Why is the CAP relevant to RL? Most RL agents attempt to solve the CAP. For example, a Q\n",
    "-learning agent attempts to learn an (optimal) value function. To do so, it needs to determine the actions that will lead to the highest value in each state.\n",
    "\n",
    "There are a few variations of the (temporal) CAP problem. For example, the structural CAP, that is, the problem of assigning credit to each structural component (which might contribute to the final outcome) of the system. [ref](https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep SARSA Cart Pole\n",
    "\n",
    "[SARSA (state-action-reward-state-action)](https://en.wikipedia.org/wiki/State–action–reward–state–action) is another Q value algorithm that resembles Q-learning quite closely:\n",
    "\n",
    "Q-learning update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma \\max_a Q_\\pi(s_{t+1}, a)\\big)\n",
    "\\end{equation}\n",
    "\n",
    "SARSA update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma Q_\\pi(s_{t+1}, a_{t+1})\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Unlike Q-learning, which is considered an *off-policy* network, SARSA is an *on-policy* algorithm. \n",
    "When Q-learning calculates the estimated future reward, it must \"guess\" the future, starting with the next action the agent will take. In Q-learning, we assume the agent will take the best possible action: $\\max_a Q_\\pi(s_{t+1}, a)$. SARSA, on the other hand, uses the action that was actually taken next in the episode we are learning from: $Q_\\pi(s_{t+1}, a_{t+1})$. In other words, SARSA learns from the next action he actually took (on policy), as opposed to what the max possible Q value for the next state was (off policy).\n",
    "\n",
    "Build an RL agent that uses SARSA to solve the Cart Pole problem. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
