{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Lenguage processing in Pytorch\n",
    "\n",
    "## Word embedding\n",
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together.\n",
    "\n",
    "Read *[Introduction to Word Embedding and Word2Vec][intro to word]*\n",
    "\n",
    "> Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.\n",
    "<br><br>Here comes the idea of generating distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other\n",
    "\n",
    "![Representation of difference between words](https://miro.medium.com/v2/resize:fit:720/format:webp/0*XMW5mf81LSHodnTi.png)\n",
    "\n",
    "[intro to word]:[https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in memory\n"
     ]
    }
   ],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('datasets/mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'datasets/mini.h5')\n",
    "    print(\"Downloading finished\")\n",
    "print(\"Dataset in memory\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an h5 file, we'll need to use the h5py package. If you followed the PyTorch installation instructions in 1A, you should have it downloaded already. Otherwise, you can install it with\n",
    "\n",
    "\\# If you environment isn't currently active, activate it: `conda activate pytorch` \n",
    "then `pip install h5py`\n",
    "\n",
    "\n",
    "You may need to re-open this notebook for the installation to take effect.\n",
    "Below, we use the package to open the mini.h5 file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their 300-d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "Random example word: /c/de/aufmachung\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "import h5py\n",
    "\n",
    "with h5py.File('datasets/mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {}\".format(all_embeddings.shape))\n",
    "\n",
    "print(\"Random example word: {}\".format(all_words[1337]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in all_words: 150875\n",
      "english_embeddings dimensions: (150875, 300)\n",
      "activated_carbon\n"
     ]
    }
   ],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"Number of English words in all_words: {0}\".format(len(english_words)))\n",
    "print(\"english_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[1337])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n",
    "\n",
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "769eaced19ad32e1b9a87390451223e7fa09e6e7964db8015ea6ac7f05d15d9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
