{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_loader.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X)-> np.ndarray:\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return StandardScaler().fit(X).transform(X)\n",
    "\n",
    "    def compute_mean_vector(self, X_std:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return X_std.mean(0)\n",
    "        \n",
    "\n",
    "    def compute_cov(self, X_std:np.ndarray, mean_vec: np.ndarray):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        X = X_std - mean_vec\n",
    "        \n",
    "        return X.T @ X / (len(X)-1)\n",
    "        \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return np.linalg.eig(cov_mat)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return np.flip(np.sort(eigen_vals/np.sum(eigen_vals)))\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs:np.ndarray, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        filtered = eig_pairs[np.flip((self.target_explained_variance-np.array(cum_var_exp)).argsort())]\n",
    "        \n",
    "        return filtered\n",
    "        \n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "#         self.feature_size = X.shape[1]\n",
    "        \n",
    "#         # your code here\n",
    "#         matrix_w = []\n",
    "        \n",
    "#         print(len(matrix_w),len(matrix_w[0]))\n",
    "#         return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Testing of implemented functions\n",
    "\n",
    "Use the below cells one by one to test each of your functions implemented above. <br>\n",
    "This should serve handy for debugging sections of your code <br>\n",
    "\n",
    "**Please Note - The hidden tests on which the actual points are awarded are different from these and usually run on a different, more complex dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec_act = pca_handler.compute_mean_vector(X_std_act)\n",
    "\n",
    "mean_vec_exp = [5.55111512, 2.77555756, 5.55111512, -5.55111512]\n",
    "\n",
    "mean_vec_act_tmp = mean_vec_act * 1e17\n",
    "\n",
    "assert pytest.approx(mean_vec_act_tmp, 0.1) == mean_vec_exp, \"Check compute_mean_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_act = pca_handler.compute_cov(X_std_act, mean_vec_act) \n",
    "\n",
    "cov_mat_exp = [[ 1.33333333, 0.97573583, 0.44021511, 0.02776305],\n",
    " [ 0.97573583, 1.33333333, 0.88156376, 0.14760488],\n",
    " [ 0.44021511, 0.88156376, 1.33333333, -0.82029039],\n",
    " [ 0.02776305, 0.14760488, -0.82029039, 1.33333333]]\n",
    "\n",
    "assert pytest.approx(cov_mat_act, 0.01) == cov_mat_exp, \"Check compute_cov function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_act, eig_vecs_act = pca_handler.compute_eigen_vector(cov_mat_act) \n",
    "\n",
    "eig_vals_exp = [2.96080083e+00, 1.80561744e+00, 5.66915059e-01, 7.86907276e-17]\n",
    "\n",
    "eig_vecs_exp = [[ 0.50989282,  0.38162981,  0.72815056,  0.25330765],\n",
    " [ 0.59707545,  0.33170546, -0.37363029, -0.62759286],\n",
    " [ 0.57599397, -0.37480162, -0.41446394,  0.59663585],\n",
    " [-0.22746684,  0.77708038, -0.3980161,   0.43126337]]\n",
    "\n",
    "assert pytest.approx(eig_vals_act, 0.01) == eig_vals_exp, \"Check compute_eigen_vector function\"\n",
    "\n",
    "for act, exp in zip(eig_vecs_act, eig_vecs_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check compute_eigen_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_handler.feature_size = X.shape[1]\n",
    "var_exp_act = pca_handler.compute_explained_variance(eig_vals_act) \n",
    "\n",
    "var_exp_exp = [0.5551501556710813, 0.33855327084133857, 0.10629657348758019, 1.475451142706682e-17]\n",
    "\n",
    "assert pytest.approx(var_exp_act, 0.01) == var_exp_exp, \"Check compute_explained_variance function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.96080083e+00, array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684]))\n",
      " (1.80561744e+00, array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038]))\n",
      " (7.86907276e-17, array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))\n",
      " (5.66915059e-01, array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ]))]\n"
     ]
    }
   ],
   "source": [
    "eig_pairs = np.array([(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "(1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "(0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])), \n",
    "(7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))],dtype=[('eig_value', float), ('eig_vector', np.ndarray)])\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "print(pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50989282  0.59707545  0.57599397 -0.22746684] [0.50989282, 0.38162981]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Check compute_weight_matrix function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m act, exp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(matrix_w_act, matrix_w_exp):\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(act[\u001b[39m1\u001b[39m],exp)\n\u001b[1;32m---> 17\u001b[0m     \u001b[39massert\u001b[39;00m pytest\u001b[39m.\u001b[39mapprox(act, \u001b[39m0.001\u001b[39m) \u001b[39m==\u001b[39m exp, \u001b[39m\"\u001b[39m\u001b[39mCheck compute_weight_matrix function\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Check compute_weight_matrix function"
     ]
    }
   ],
   "source": [
    "eig_pairs = np.array([(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "(1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "(0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])), \n",
    "(7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))],dtype=[('eig_value', float), ('eig_vector', np.ndarray)])\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "\n",
    "matrix_w_exp = [[0.50989282, 0.38162981], \n",
    "                [0.59707545, 0.33170546], \n",
    "                [0.57599397, -0.37480162], \n",
    "                [-0.22746684, 0.77708038]]\n",
    "\n",
    "matrix_w_act = pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp)\n",
    "\n",
    "for act, exp in zip(matrix_w_act, matrix_w_exp):\n",
    "    print(act[1],exp)\n",
    "    assert pytest.approx(act, 0.001) == exp, \"Check compute_weight_matrix function\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
