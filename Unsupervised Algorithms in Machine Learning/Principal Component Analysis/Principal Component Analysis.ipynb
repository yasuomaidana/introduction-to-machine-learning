{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_loader.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X)-> np.ndarray:\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return StandardScaler().fit(X).transform(X)\n",
    "\n",
    "    def compute_mean_vector(self, X_std:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return X_std.mean(0)\n",
    "        \n",
    "\n",
    "    def compute_cov(self, X_std:np.ndarray, mean_vec: np.ndarray):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        X = X_std - mean_vec\n",
    "        \n",
    "        return X.T @ X / (len(X)-1)\n",
    "        \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return np.linalg.eig(cov_mat)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return np.flip(np.sort(eigen_vals/np.sum(eigen_vals)))\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs:np.ndarray, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        variance_difference = self.target_explained_variance-np.array(cum_var_exp)\n",
    "        valid_indexes = variance_difference[variance_difference>0]\n",
    "        sorted_indexes = np.flip(valid_indexes.argsort())\n",
    "        sorted_eig_pairs = eig_pairs[sorted_indexes]\n",
    "\n",
    "        return np.array([np.array(eigen_vector) for eigen_vector in sorted_eig_pairs['eig_vector']]).T\n",
    "        \n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # your code here\n",
    "        X_std_act = self.standardize(X)\n",
    "        mean_vec_act = self.compute_mean_vector(X_std_act)\n",
    "        cov_mat_act = self.compute_cov(X_std_act, mean_vec_act)\n",
    "        eig_vals_act, eig_vecs_act = self.compute_eigen_vector(cov_mat_act)\n",
    "        var_exp_act = self.compute_explained_variance(eig_vals_act) \n",
    "        eig_pairs = np.array([(vals,vect) for vals, vect in zip(eig_vals_act, eig_vecs_act.T)],dtype=[('eig_value', '<f8'), ('eig_vector', 'O')])\n",
    "        accum_var = self.cumulative_sum(var_exp_act)\n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs,accum_var)\n",
    "        \n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_std_act, matrix_w=matrix_w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 409\n"
     ]
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Testing of implemented functions\n",
    "\n",
    "Use the below cells one by one to test each of your functions implemented above. <br>\n",
    "This should serve handy for debugging sections of your code <br>\n",
    "\n",
    "**Please Note - The hidden tests on which the actual points are awarded are different from these and usually run on a different, more complex dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec_act = pca_handler.compute_mean_vector(X_std_act)\n",
    "\n",
    "mean_vec_exp = [5.55111512, 2.77555756, 5.55111512, -5.55111512]\n",
    "\n",
    "mean_vec_act_tmp = mean_vec_act * 1e17\n",
    "\n",
    "assert pytest.approx(mean_vec_act_tmp, 0.1) == mean_vec_exp, \"Check compute_mean_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_act = pca_handler.compute_cov(X_std_act, mean_vec_act) \n",
    "\n",
    "cov_mat_exp = [[ 1.33333333, 0.97573583, 0.44021511, 0.02776305],\n",
    " [ 0.97573583, 1.33333333, 0.88156376, 0.14760488],\n",
    " [ 0.44021511, 0.88156376, 1.33333333, -0.82029039],\n",
    " [ 0.02776305, 0.14760488, -0.82029039, 1.33333333]]\n",
    "\n",
    "assert pytest.approx(cov_mat_act, 0.01) == cov_mat_exp, \"Check compute_cov function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_act, eig_vecs_act = pca_handler.compute_eigen_vector(cov_mat_act) \n",
    "\n",
    "eig_vals_exp = [2.96080083e+00, 1.80561744e+00, 5.66915059e-01, 7.86907276e-17]\n",
    "\n",
    "eig_vecs_exp = [[ 0.50989282,  0.38162981,  0.72815056,  0.25330765],\n",
    " [ 0.59707545,  0.33170546, -0.37363029, -0.62759286],\n",
    " [ 0.57599397, -0.37480162, -0.41446394,  0.59663585],\n",
    " [-0.22746684,  0.77708038, -0.3980161,   0.43126337]]\n",
    "\n",
    "assert pytest.approx(eig_vals_act, 0.01) == eig_vals_exp, \"Check compute_eigen_vector function\"\n",
    "\n",
    "for act, exp in zip(eig_vecs_act, eig_vecs_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check compute_eigen_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_handler.feature_size = X.shape[1]\n",
    "var_exp_act = pca_handler.compute_explained_variance(eig_vals_act) \n",
    "\n",
    "var_exp_exp = [0.5551501556710813, 0.33855327084133857, 0.10629657348758019, 1.475451142706682e-17]\n",
    "\n",
    "assert pytest.approx(var_exp_act, 0.01) == var_exp_exp, \"Check compute_explained_variance function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs = np.array([(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "(1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "(0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])), \n",
    "(7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))],dtype=[('eig_value', '<f8'), ('eig_vector', 'O')])\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "\n",
    "matrix_w_exp = [[0.50989282, 0.38162981], \n",
    "                [0.59707545, 0.33170546], \n",
    "                [0.57599397, -0.37480162], \n",
    "                [-0.22746684, 0.77708038]]\n",
    "\n",
    "matrix_w_act = pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp)\n",
    "\n",
    "for act, exp in zip(matrix_w_act, matrix_w_exp):\n",
    "    assert pytest.approx(act, 0.001) == exp, \"Check compute_weight_matrix function\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Comparison with Sklearn\n",
    "\n",
    "The below cells should help you compare the output from your implementation against the sklearn implementation with a similar configuration. This is solely to help you validate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50978142,  1.90389377],\n",
       "       [-2.00244126, -0.68224687],\n",
       "       [-0.57630716, -0.07213549],\n",
       "       [ 2.068967  , -1.14951142]])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this cell to verify against the sklearn implementation given in the next cell\n",
    "\n",
    "pca_handler.transform_data(X_std=X_std_act, matrix_w=pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: [[ 0.39  1.07  0.06  0.79]\n",
      " [-1.15 -0.51 -0.21 -0.7 ]\n",
      " [-1.36  0.57  0.37  0.09]\n",
      " [ 0.06  1.04  0.99 -1.78]]\n",
      "\n",
      "Scaled data: [[ 1.20216033  0.82525828 -0.54269609  1.24564656]\n",
      " [-0.84350476 -1.64660539 -1.14693504 -0.31402854]\n",
      " [-1.1224591   0.04302294  0.15105974  0.51291329]\n",
      " [ 0.76380353  0.77832416  1.53857139 -1.4445313 ]]\n",
      "\n",
      "Transformed Data [[ 0.50978142  1.90389378]\n",
      " [-2.00244127 -0.68224688]\n",
      " [-0.57630715 -0.07213548]\n",
      " [ 2.068967   -1.14951141]]\n"
     ]
    }
   ],
   "source": [
    "# Sklearn implementation to compare your results\n",
    "\n",
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA as pca1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling=StandardScaler()\n",
    " \n",
    "# Use fit and transform method\n",
    "# You may change the variable X if needed to verify against a different dataset\n",
    "print(\"Sample data:\", X)\n",
    "scaling.fit(X)\n",
    "Scaled_data=scaling.transform(X)\n",
    "print(\"\\nScaled data:\", Scaled_data)\n",
    " \n",
    "# Set the n_components=3\n",
    "principal=pca1(n_components=2)\n",
    "principal.fit(Scaled_data)\n",
    "x=principal.transform(Scaled_data)\n",
    " \n",
    "# Check the dimensions of data after PCA\n",
    "print(\"\\nTransformed Data\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.50978142,  1.90389378],\n",
       "       [-2.00244127, -0.68224688],\n",
       "       [-0.57630715, -0.07213548],\n",
       "       [ 2.068967  , -1.14951141]])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "pca_handler.fit(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
