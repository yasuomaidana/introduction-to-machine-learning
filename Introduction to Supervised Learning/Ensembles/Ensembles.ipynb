{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods (adaBoost, random forests) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the helper code below to create a training and validation set of threes and eights from the MNIST dataset. There is also a helper function to display digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreesandEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST 3s and 8s data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        x_train, y_train, x_test, y_test = pickle.load(f)\n",
    "                \n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.x_train = x_train[np.logical_or(y_train== 3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or(y_train== 3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.x_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.x_train = self.x_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.x_test = x_test[np.logical_or(y_test== 3, y_test == 8), :]\n",
    "        self.y_test = y_test[np.logical_or(y_test== 3, y_test == 8)]\n",
    "        self.y_test = np.array([1 if y == 8 else -1 for y in self.y_test])\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "def view_digit(ex, label=None, feature=None):\n",
    "    \"\"\"\n",
    "    function to plot digit examples \n",
    "    \"\"\"\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    img = ex.reshape(21,21)\n",
    "    col = np.dstack((img, img, img))\n",
    "    if feature is not None: col[feature[0]//21, feature[0]%21, :] = [1, 0, 0]\n",
    "    plt.imshow(col)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "data = ThreesandEights(\"data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_digit(data.x_train[0], data.y_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Adaboost Classifier to classify MNIST digits 3 and 8."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `AdaBoost` class below, implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "Recall that the AdaBoost algorithm is as follows: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) Fit kth weak learner to training data with weights w`\n",
    "\n",
    "$~~~~~~~$ `    b) Computed weighted error errk for the kth weak learner (Check Adaboost slides for formula).`\n",
    "\n",
    "$~~~~~~~$ `    c) compute vote weight alpha[k] = 0.5 ln ((1-errk)/errk))`\n",
    "\n",
    "$~~~~~~~$ `    d) update training example weights w[i] *= exp[-alpha[k] y[i] h[k](x[i])]`\n",
    "\n",
    "$~~~~~~~$ `    e) normalize training weights so they sum to 1`\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=3), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "        # you don't have to use sklearn's fit function, \n",
    "        # but it is probably the easiest way \n",
    "\n",
    "        # w = np.ones(len(y_train))\n",
    "        # w /= np.sum(w) \n",
    "        # for loop\n",
    "        #   h = clone(self.base)\n",
    "        #   h.fit(X_train, y_train, sample_weight=w)\n",
    "        #   ...\n",
    "        #\n",
    "        #\n",
    "        #   ...\n",
    "        #   Save alpha and learner\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        # your code here\n",
    "        w = np.ones(len(y_train))\n",
    "        w /= np.sum(w)\n",
    "        for i in range(self.n_learners):\n",
    "            h = clone(self.base)\n",
    "            h.fit(X_train,y_train,w)\n",
    "\n",
    "            yh = h.predict(X_train)\n",
    "            \n",
    "            e_b = self.error_rate(y_train,yh,w)\n",
    "\n",
    "            coef_b = 1/2 * np.log((1-e_b)/e_b)\n",
    "            \n",
    "            w = w * np.exp(-coef_b*y_train*yh)\n",
    "            #w /= np.sum(w)\n",
    "        \n",
    "            self.alpha[i] = coef_b\n",
    "            self.learners.append(h)\n",
    "        \n",
    "        return self  \n",
    "            \n",
    "    def error_rate(self, y_true, y_pred, weights):\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Implement the weighted error rate\n",
    "        # =================================================================\n",
    "        # your code here\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        not_equal_index = np.where(y_true != y_pred)\n",
    "        \n",
    "        return np.sum(weights[not_equal_index])/np.sum(weights)\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        yhat = np.zeros(X.shape[0])\n",
    "        \n",
    "        # your code here\n",
    "        for alpha, hypothesis in zip(self.alpha, self.learners):\n",
    "            yhat += alpha*hypothesis.predict(X)\n",
    "        return np.sign(yhat)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        return np.array(scores)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test for Adaboost error rate function. \n",
    "import pytest\n",
    "\n",
    "y_true = [-1, 1, 1, -1, 1, -1, -1]\n",
    "y_pred = [-1, 1, 1, 1, 1, -1, 1]\n",
    "w = np.ones(len(y_true))\n",
    "w /= np.sum(w)\n",
    "\n",
    "clf = AdaBoost() \n",
    "err_rate = clf.error_rate(y_true, y_pred, w)\n",
    "assert pytest.approx(err_rate, 0.01) == 0.2857, \"Check the error_rate function.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test for Adaboost fit function. \n",
    "\n",
    "sample_data = np.load('data/train.npz') \n",
    "sample_X = sample_data['X']\n",
    "sample_y = sample_data['y']\n",
    "test_model = AdaBoost(n_learners=5).fit(sample_X,sample_y)\n",
    "t_alpha = [1.94591015, 2.14179328, 2.48490665, 1.94679667, 2.22627839]\n",
    "assert pytest.approx(test_model.alpha, 0.01) == t_alpha, \"Check the fit function\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fit function to fit the Adaboost classifier with 150 base decision tree stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(data.y_train))\n",
    "clf = AdaBoost(n_learners=150, base = DecisionTreeClassifier(max_depth=1)).fit(data.x_train,data.y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples. You can test out the predictions in the cell below.   **Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "Just print out your predictions on the training set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ... -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "train_predict = clf.predict(data.x_train)\n",
    "print(train_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
