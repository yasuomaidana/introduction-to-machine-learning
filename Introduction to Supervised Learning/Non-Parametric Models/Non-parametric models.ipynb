{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# importing all the required libraries\n",
    "\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a K- Nearest neighbours classifier for handwritten digit recognition \n",
    "\n",
    "In this problem you will complete some code to build a k-nearest neighbour classifier to classify images of handwritten digits (0-9). For this purpose we will use a famous open-source dataset of handwritten digits called the MNIST that is ubiquitously used for testing a number of classification algorithms in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_import:\n",
    "    \"\"\"\n",
    "    sets up MNIST dataset from OpenML \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        df = pd.read_csv(\"data/mnist_784.csv\")\n",
    "        \n",
    "        # Create arrays for the features and the response variable\n",
    "        # store for use later \n",
    "        y = df['class'].values\n",
    "        X = df.drop('class', axis=1).values\n",
    "         \n",
    "        # Convert the labels to numeric labels\n",
    "        y = np.array(pd.to_numeric(y))\n",
    "        \n",
    "        # create training and validation sets \n",
    "        self.train_x, self.train_y = X[:5000,:], y[:5000]\n",
    "        self.val_x, self.val_y = X[5000:6000,:], y[5000:6000]\n",
    "        \n",
    "data = MNIST_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(28,28), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a particular digit using the above function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_index = 9\n",
    "# your code here\n",
    "view_digit(data.train_x[training_index],data.train_y[training_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code in the following cell to determine the following quantities:\n",
    "   - Number of pixels in each image\n",
    "   - Number of examples in the training set\n",
    "   - Number of examples in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "# Here are the numbers you need to provide here:\n",
    "num_training_examples = 0\n",
    "num_test_examples = 0\n",
    "pixels_per_image = 0\n",
    "\n",
    "# your code here\n",
    "num_training_examples = len(data.train_x)\n",
    "num_test_examples = len(data.val_x)\n",
    "pixels_per_image = data.train_x[0].shape[0]\n",
    "\n",
    "print(num_training_examples)\n",
    "print(num_test_examples)\n",
    "print(pixels_per_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the class above to implement a KNN classifier.  There are three methods that you need to complete: \n",
    "\n",
    "- `predict`: Given an $m \\times p$ matrix of validation data with $m$ examples each with $p$ features, return a length-$m$ vector of predicted labels by calling the `classify` function on each example. \n",
    "- `classify`: Given a single query example with $p$ features, return its predicted class label as an integer using KNN by calling the `majority` function. \n",
    "- `majority`: Given an array of indices into the training set corresponding to the $K$ training examples that are nearest to the query point, return the majority label as an integer.  If there is a tie for the majority label using $K$ nearest neighbors, reduce $K$ by 1 and try again.  Continue reducing $K$ until there is a winning label. \n",
    "\n",
    "**Notes**: \n",
    "- Don't even think about implementing nearest-neighbor search or any distance metrics yourself.  Instead, go read the documentation for Scikit-Learn's [BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) object.  You will find that its implemented [query](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree.query) method can do most of the heavy lifting for you. \n",
    "- Do not use Scikit-Learn's KNeighborsClassifier in this problem.  We're implementing this ourselves. \n",
    "- Use the visible test cases to validate your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, x_train, y_train, K=5):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param x_train: numpy array with shape (n_rows,1)- e.g. [[1,2],[3,4]]\n",
    "        :param y_train: numpy array with shape (n_rows,)- e.g. [1,-1]\n",
    "        :param K: The number of nearest points to consider in classification\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import and build the BallTree on training features \n",
    "        from sklearn.neighbors import BallTree\n",
    "        self.balltree = BallTree(x_train)\n",
    "        \n",
    "        # Cache training labels and parameter K \n",
    "        self.y_train = y_train\n",
    "        self.K = K \n",
    "        \n",
    "        \n",
    "    def majority(self, neighbor_indices, neighbor_distances=None):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param neighbor_indices: The indices of the K nearest neighbors in self.X_train \n",
    "        :param neighbor_distances: Corresponding distances from query point to K nearest neighbors. \n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "            \n",
    "        \n",
    "    def classify(self, x):\n",
    "        \"\"\"\n",
    "        Given a query point, return the predicted label \n",
    "        \n",
    "        :param x: a query point stored as an ndarray  \n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an ndarray of query points, return yhat, an ndarray of predictions \n",
    "\n",
    "        :param X: an (m x p) dimension ndarray of points to predict labels for \n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
